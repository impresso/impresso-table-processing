\chapter{Introduction}
\section{Context and motivation}
For more than four centuries -- the first newspaper published in the form we know today dates back to 1605 \citep{weber_strassburg_2006} -- newspapers have accompanied modern societies. Periodically and locally published, generally massively printed and easily accessible, newspapers represent extremely valuable historical sources for this time period. The diversity of their content, mostly news of all kinds, from international to local events but also articles on every conceivable subjects, has remained stable over the years. The same can not be said about their format and layouts, which changed a lot over time, some even going fully online now. Collected and stored by public libraries and archives over the years, newspapers have recently been subject to massive digitization, with the objective to make them more accessible but also, and perhaps more importantly, easily processable. \\

Thanks to advances in document and information processing, notably Computer Vision (CV) and Natural Language Processing (NLP), this mass of information is becoming more and more searchable and exploitable. CV algorithms are used to convert visual facsimiles into text -- which refers to the task of Optical Character Recognition (OCR) -- as well as to detect regions of interest to identify e.g. articles, mastheads or photographs -- the task of Optical Layout Recognition (OLR) --. Outputs of such processes can in turn be processed by NLP algorithms to e.g. extract and link entities of interest or identify thematics. Put together in a well-thought graphical user interface, these tools constitute a formidable arsenal at the disposal of the historians, allowing them to easily query a quantity of information so important that several lifetimes would have been necessary with the usual means.\\

This Master's thesis, in line with that digitization effort, is part of a larger project called \textit{impresso - Media Monitoring of the Past} \footnote{\textit{impresso}. Media Monitoring of the Past. Supported by the Swiss National Science Foundation under grant CR- SII5\_173719, 2019. \url{https://impresso-project.ch}.}, which aims to semantically enrich 200 years of newspapers archives by applying a series of NLP techniques. To date, this project has collected the archives of 76 newspapers from Switzerland and Luxembourg dating back to 1738 and representing over 5,400,000 scanned pages. The source material comes from Swiss and Luxembourg national libraries and corresponds to the facsimiles and OCR (and sometimes OLR) outputs these libraries produced. Unfortunately, further automatic processing of these sources is severely hampered by the sometimes poor quality of the legacy OCR processes, which were often produced long ago with now outdated algorithms. In particular, it is known that specific parts of newspapers suffered greatly from bad OCR due to their unconventional design, often consisting of tables. \\

Newspaper layouts are a case in point of the difficulty of the \textit{document layout analysis} task -- the process of detecting and categorizing the regions of interest in a scanned document --, which OLR algorithms attempt to address. Indeed, the variety of elements that can be found in terms of contents and layouts within a newspaper page is greater than in other types of document types (e.g. novels), and when considering \textit{historical} newspapers, their diachronic properties in terms of layout and language makes the task even harder. OLR and OCR are the first steps of any digitized newspaper text processing pipeline, as is the case with \textit{impresso}: NLP processes are applied afterwards, i.e. on transcribed text pieces identified as belonging to the same segment or item (e.g. paragraphs of an article). Beyond transcription and item segmentation, it may be worthwhile to leverage document layout analysis algorithms to classify these segments, for two reasons. First, in order to tune NLP approaches according to item types, or to filter out unwanted or noisy items in terms of OCR. Second, it would allow faceted search as well as quantitative studies on newspaper items. Indeed, historians would greatly benefit from a tool capable of providing a more refined classification of newspaper items. Querying historical databases could not only be done at the text level but also at the layout level by taking into account the type of items. This would be great because newspaper items could be queried for what they are, providing an alternative to information retrieval by keywords alone and thus the reliance on OCR. Quantitative analyses around specific item types could be undertaken, which means that their evolution could be observed e.g. over time, or per journal. 

\section{Objective and proposition}
The objective of this Master's project is to develop and evaluate a pipeline for the accurate detection and fine-grained semantic classification of tables in newspapers using a large, already annotated dataset. A table is a recurring item in newspapers, that typically contains factual data in an arrangement of cells, making it a valuable layout object for its conciseness. Examples include weather reports, stock exchange tables, or movie schedules, which often consist of a few words or names associated with a number or a time. Tables present several challenges as they can have a wide variety of semantic class due to their ubiquitous use, but also because their appearance can be very confusing as it may change considerably across time and sources. Nonetheless, previous work (\citet{ares_oliveira_dhsegment_2018}, \citet{barman_combining_2021}) has shown that deep learning architectures should be well-suited to meet these objectives in the context of historical documents and newspapers, provided there is sufficient training data of quality. \\
Due to unforeseen issues in the dataset to be used, where the consistency of its ground truth differed from the original expectations -- which typically occurs only once that the data is actually manipulated --, some additional lines of research had to be considered. Indeed, an investigation of the extent of the inconsistency in the dataset and whether it can be addressed became necessary, as well as an evaluation of the capacity for deep learning models to detect and classify tables in newspapers in a context of imperfect data.\\

In this project, we conducted a statistical exploration of the dataset used as the basis for all subsequently constructed datasets, namely the collection of newspapers of the National Library of Luxembourg, with the objectives to discover its main characteristics as well as to understand its inconsistencies and limitations. With regard to table detection, we experimented with and evaluated two image segmentation models to compare their performances when trained on datasets of different consistencies. These datasets were constructed through manual annotations, and the application of a filtering strategy based on the identification of problematic data points. For the task of table classification, we experimented and evaluated three classifiers based on different data modalities including text, layout and image information. Their performance is assessed on a manually annotated dataset as well as on a dataset that has been augmented by automatic labelling based on visual similarity.\\

The remainder of this thesis is organized as follows. Chapter \ref{literature_review} introduces the various sources used for this thesis and provides an overview of the current state of research regarding table detection and classification in document images. It also introduces some of the key concepts needed for the rest of the thesis. Chapter \ref{datasets_construction} covers the construction of the many datasets necessary to address the problems of table detection and classification. Chapter \ref{models} introduces the models used, and outlines the methodology followed for their evaluation. Chapter \ref{experiments} presents the experiments performed and analyses the results obtained. Finally, Chapter \ref{conclusion} summarizes the findings of this project, and proposes directions for further research.