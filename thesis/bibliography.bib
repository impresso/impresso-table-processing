
@article{barman_combining_2021,
	title = {Combining Visual and Textual Features for Semantic Segmentation of Historical Newspapers},
	doi = {10.5281/zenodo.4065271},
	abstract = {The massive amounts of digitized historical documents acquired over the last decades naturally lend themselves to automatic processing and exploration. Research work seeking to automatically process facsimiles and extract information thereby are multiplying with, as a first essential step, document layout analysis. Although the identification and categorization of segments of interest in document images have seen significant progress over the last years thanks to deep learning techniques, many challenges remain with, among others, the use of more fine-grained segmentation typologies and the consideration of complex, heterogeneous documents such as historical newspapers. Besides, most approaches consider visual features only, ignoring textual signal. We introduce a multimodal neural model for the semantic segmentation of historical newspapers that directly combines visual features at pixel level with text embedding maps derived from, potentially noisy, {OCR} output. Based on a series of experiments on diachronic Swiss and Luxembourgish newspapers, we investigate the predictive power of visual and textual features and their capacity to generalize across time and sources. Results show consistent improvement of multimodal models in comparison to a strong visual baseline, as well as better robustness to the wide variety of our material},
	journaltitle = {Journal of Data Mining \& Digital Humanities},
	editor = {Barman, Raphaël and Ehrmann, Maud and Clematide, Simon and Ares Oliveira, Sofia and Kaplan, Frédéric},
	date = {2021},
	keywords = {deep learning, digital humanitites, historical newspapers, image segmentation, multimodal learning},
	file = {Barman et al. - 2021 - Combining Visual and Textual Features for Semantic.pdf:/Users/assa/Zotero/storage/D66HEEMR/Barman et al. - 2021 - Combining Visual and Textual Features for Semantic.pdf:application/pdf},
}

@inproceedings{ares_oliveira_dhsegment_2018,
	title = {{dhSegment}: A Generic Deep-Learning Approach for Document Segmentation},
	doi = {10.1109/ICFHR-2018.2018.00011},
	shorttitle = {{dhSegment}},
	abstract = {In recent years there have been multiple successful attempts tackling document processing problems separately by designing task specific hand-tuned strategies. We argue that the diversity of historical document processing tasks prohibits to solve them one at a time and shows a need for designing generic approaches in order to handle the variability of historical series. In this paper, we address multiple tasks simultaneously such as page extraction, baseline extraction, layout analysis or multiple typologies of illustrations and photograph extraction. We propose an open-source implementation of a {CNN}-based pixel-wise predictor coupled with task dependent post-processing blocks. We show that a single {CNN}-architecture can be used across tasks with competitive results. Moreover most of the task-specific post-precessing steps can be decomposed in a small number of simple and standard reusable operations, adding to the flexibility of our approach.},
	eventtitle = {2018 16th International Conference on Frontiers in Handwriting Recognition ({ICFHR})},
	pages = {7--12},
	booktitle = {2018 16th International Conference on Frontiers in Handwriting Recognition ({ICFHR})},
	author = {Ares Oliveira, Sofia and Seguin, Benoit and Kaplan, Frederic},
	date = {2018-08},
	keywords = {deep learning, document layout analysis, document segmentation, historical document processing, Image segmentation, Layout, neural network, Neural networks, Standards, Task analysis, Text analysis, Training},
	file = {Ares Oliveira et al. - 2018 - dhSegment A Generic Deep-Learning Approach for Do.pdf:/Users/assa/Zotero/storage/3J5YNJ3R/Ares Oliveira et al. - 2018 - dhSegment A Generic Deep-Learning Approach for Do.pdf:application/pdf},
}

@inproceedings{noauthor_bir_2021,
	location = {Lausanne},
	title = {The {BIR} database – Identifying typographic emphasis in list-like historical documents},
	booktitle = {{HIP}'21},
	date = {2021},
	file = {2021 - The BIR database – Identifying typographic emphasi.pdf:/Users/assa/Zotero/storage/CPJE7FYB/2021 - The BIR database – Identifying typographic emphasi.pdf:application/pdf},
}

@inproceedings{zhang_vsr_2021,
	location = {Cham},
	title = {{VSR}: A Unified Framework for Document Layout Analysis Combining Vision, Semantics and Relations},
	isbn = {978-3-030-86549-8},
	doi = {10.1007/978-3-030-86549-8_8},
	series = {Lecture Notes in Computer Science},
	shorttitle = {{VSR}},
	abstract = {Document layout analysis is crucial for understanding document structures. On this task, vision and semantics of documents, and relations between layout components contribute to the understanding process. Though many works have been proposed to exploit the above information, they show unsatisfactory results. {NLP}-based methods model layout analysis as a sequence labeling task and show insufficient capabilities in layout modeling. {CV}-based methods model layout analysis as a detection or segmentation task, but bear limitations of inefficient modality fusion and lack of relation modeling between layout components. To address the above limitations, we propose a unified framework {VSR} for document layout analysis, combining vision, semantics and relations. {VSR} supports both {NLP}-based and {CV}-based methods. Specifically, we first introduce vision through document image and semantics through text embedding maps. Then, modality-specific visual and semantic features are extracted using a two-stream network, which are adaptively fused to make full use of complementary information. Finally, given component candidates, a relation module based on graph neural network is incorported to model relations between components and output final results. On three popular benchmarks, {VSR} outperforms previous models by large margins. Code will be released soon.},
	pages = {115--130},
	booktitle = {Document Analysis and Recognition – {ICDAR} 2021},
	publisher = {Springer International Publishing},
	author = {Zhang, Peng and Li, Can and Qiao, Liang and Cheng, Zhanzhan and Pu, Shiliang and Niu, Yi and Wu, Fei},
	editor = {Lladós, Josep and Lopresti, Daniel and Uchida, Seiichi},
	date = {2021},
	langid = {english},
	keywords = {Document layout analysis, Relations, Semantics, Vision},
	file = {Springer Full Text PDF:/Users/assa/Zotero/storage/AX8N9J6T/Zhang et al. - 2021 - VSR A Unified Framework for Document Layout Analy.pdf:application/pdf},
}

@inproceedings{garncarek_lambert_2021,
	location = {Cham},
	title = {{LAMBERT}: Layout-Aware Language Modeling for Information Extraction},
	isbn = {978-3-030-86549-8},
	doi = {10.1007/978-3-030-86549-8_34},
	series = {Lecture Notes in Computer Science},
	shorttitle = {{LAMBERT}},
	abstract = {We introduce a simple new approach to the problem of understanding documents where non-trivial layout influences the local semantics. To this end, we modify the Transformer encoder architecture in a way that allows it to use layout features obtained from an {OCR} system, without the need to re-learn language semantics from scratch. We only augment the input of the model with the coordinates of token bounding boxes, avoiding, in this way, the use of raw images. This leads to a layout-aware language model which can then be fine-tuned on downstream tasks.The model is evaluated on an end-to-end information extraction task using four publicly available datasets: Kleister {NDA}, Kleister Charity, {SROIE} and {CORD}. We show that our model achieves superior performance on datasets consisting of visually rich documents, while also outperforming the baseline {RoBERTa} on documents with flat layout ({NDA} {\textbackslash}(F\_\{1\}{\textbackslash}) increase from 78.50 to 80.42). Our solution ranked first on the public leaderboard for the Key Information Extraction from the {SROIE} dataset, improving the {SOTA} {\textbackslash}(F\_\{1\}{\textbackslash})-score from 97.81 to 98.17.},
	pages = {532--547},
	booktitle = {Document Analysis and Recognition – {ICDAR} 2021},
	publisher = {Springer International Publishing},
	author = {Garncarek, Łukasz and Powalski, Rafał and Stanisławek, Tomasz and Topolski, Bartosz and Halama, Piotr and Turski, Michał and Graliński, Filip},
	editor = {Lladós, Josep and Lopresti, Daniel and Uchida, Seiichi},
	date = {2021},
	langid = {english},
	keywords = {Layout, Document understanding, Key information extraction, Language model, Transformer, Visually rich document},
	file = {Springer Full Text PDF:/Users/assa/Zotero/storage/NJ2Q6ZXH/Garncarek et al. - 2021 - LAMBERT Layout-Aware Language Modeling for Inform.pdf:application/pdf},
}

@inproceedings{shen_layoutparser_2021,
	location = {Cham},
	title = {{LayoutParser}: A Unified Toolkit for Deep Learning Based Document Image Analysis},
	isbn = {978-3-030-86549-8},
	doi = {10.1007/978-3-030-86549-8_9},
	series = {Lecture Notes in Computer Science},
	shorttitle = {{LayoutParser}},
	abstract = {Recent advances in document image analysis ({DIA}) have been primarily driven by the application of neural networks. Ideally, research outcomes could be easily deployed in production and extended for further investigation. However, various factors like loosely organized codebases and sophisticated model configurations complicate the easy reuse of important innovations by a wide audience. Though there have been on-going efforts to improve reusability and simplify deep learning ({DL}) model development in disciplines like natural language processing and computer vision, none of them are optimized for challenges in the domain of {DIA}. This represents a major gap in the existing toolkit, as {DIA} is central to academic research across a wide range of disciplines in the social sciences and humanities. This paper introduces {LayoutParser}, an open-source library for streamlining the usage of {DL} in {DIA} research and applications. The core {LayoutParser} library comes with a set of simple and intuitive interfaces for applying and customizing {DL} models for layout detection, character recognition, and many other document processing tasks. To promote extensibility, {LayoutParser} also incorporates a community platform for sharing both pre-trained models and full document digitization pipelines. We demonstrate that {LayoutParser} is helpful for both lightweight and large-scale digitization pipelines in real-word use cases. The library is publicly available at https://layout-parser.github.io.},
	pages = {131--146},
	booktitle = {Document Analysis and Recognition – {ICDAR} 2021},
	publisher = {Springer International Publishing},
	author = {Shen, Zejiang and Zhang, Ruochen and Dell, Melissa and Lee, Benjamin Charles Germain and Carlson, Jacob and Li, Weining},
	editor = {Lladós, Josep and Lopresti, Daniel and Uchida, Seiichi},
	date = {2021},
	langid = {english},
	keywords = {Character recognition, Deep learning, Document image analysis, Layout analysis, Open source library, Toolkit},
	file = {Springer Full Text PDF:/Users/assa/Zotero/storage/G5C9W6E3/Shen et al. - 2021 - LayoutParser A Unified Toolkit for Deep Learning .pdf:application/pdf},
}

@article{prasad_cascadetabnet_2020,
	title = {{CascadeTabNet}: An approach for end to end table detection and structure recognition from image-based documents},
	url = {http://arxiv.org/abs/2004.12629},
	shorttitle = {{CascadeTabNet}},
	abstract = {An automatic table recognition method for interpretation of tabular data in document images majorly involves solving two problems of table detection and table structure recognition. The prior work involved solving both problems independently using two separate approaches. More recent works signify the use of deep learning-based solutions while also attempting to design an end to end solution. In this paper, we present an improved deep learning-based end to end approach for solving both problems of table detection and structure recognition using a single Convolution Neural Network ({CNN}) model. We propose {CascadeTabNet}: a Cascade mask Region-based {CNN} High-Resolution Network (Cascade mask R-{CNN} {HRNet}) based model that detects the regions of tables and recognizes the structural body cells from the detected tables at the same time. We evaluate our results on {ICDAR} 2013, {ICDAR} 2019 and {TableBank} public datasets. We achieved 3rd rank in {ICDAR} 2019 post-competition results for table detection while attaining the best accuracy results for the {ICDAR} 2013 and {TableBank} dataset. We also attain the highest accuracy results on the {ICDAR} 2019 table structure recognition dataset. Additionally, we demonstrate effective transfer learning and image augmentation techniques that enable {CNNs} to achieve very accurate table detection results. Code and dataset has been made available at: https://github.com/{DevashishPrasad}/{CascadeTabNet}},
	journaltitle = {{arXiv}:2004.12629 [cs]},
	author = {Prasad, Devashish and Gadpal, Ayan and Kapadni, Kshitij and Visave, Manish and Sultanpure, Kavita},
	urldate = {2021-09-15},
	date = {2020-05-28},
	eprinttype = {arxiv},
	eprint = {2004.12629},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/assa/Zotero/storage/QR7AWHGF/Prasad et al. - 2020 - CascadeTabNet An approach for end to end table de.pdf:application/pdf;arXiv.org Snapshot:/Users/assa/Zotero/storage/BUFSAC89/2004.html:text/html},
}

@article{he_mask_2018,
	title = {Mask R-{CNN}},
	url = {http://arxiv.org/abs/1703.06870},
	abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-{CNN}, extends Faster R-{CNN} by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-{CNN} is simple to train and adds only a small overhead to Faster R-{CNN}, running at 5 fps. Moreover, Mask R-{CNN} is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the {COCO} suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-{CNN} outperforms all existing, single-model entries on every task, including the {COCO} 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
	journaltitle = {{arXiv}:1703.06870 [cs]},
	author = {He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross},
	urldate = {2021-09-17},
	date = {2018-01-24},
	eprinttype = {arxiv},
	eprint = {1703.06870},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/assa/Zotero/storage/6R733HJ4/He et al. - 2018 - Mask R-CNN.pdf:application/pdf;arXiv.org Snapshot:/Users/assa/Zotero/storage/EHM49DKS/1703.html:text/html},
}

@inproceedings{quoc_comparing_2020,
	title = {Comparing U-Net Convolutional Network with Mask R-{CNN} in Agricultural Area Segmentation on Satellite Images},
	doi = {10.1109/NICS51282.2020.9335856},
	abstract = {Deep learning is the fastest-growing trend in statistical analysis of remote sensing data. Deep learning models are used for information processing of spectral steps, identification statistics, segmentation and classification of the objects in satellite images, etc. Image segmentation could help to make the object statistics more accurate by separating the objects from the background. In this paper, we propose knowledge of Mask R-{CNN} and U-Net in satellite imagery segmentation, and we also make an experiment for these models to show the appropriateness in this field. Experimental result of the mean average precision ({mAP}) on dataset of Vietnam satellite images is 95.21\% for Mask R-{CNN} and 92.69\% for U-Net.},
	eventtitle = {2020 7th {NAFOSTED} Conference on Information and Computer Science ({NICS})},
	pages = {124--129},
	booktitle = {2020 7th {NAFOSTED} Conference on Information and Computer Science ({NICS})},
	author = {Quoc, Thinh Tran Pham and Linh, Tam Tran and Minh, Thu Nguyen Tran},
	date = {2020-11},
	keywords = {Image segmentation, Deep learning, agricultural areas, Market research, Mask R-{CNN}, Object recognition, Remote sensing, satellite images, Satellites, segmentation, Statistical analysis, U-Net},
	file = {IEEE Xplore Full Text PDF:/Users/assa/Zotero/storage/FG5KNVG5/Quoc et al. - 2020 - Comparing U-Net Convolutional Network with Mask R-.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/assa/Zotero/storage/4M6I3JP9/9335856.html:text/html},
}

@article{xu_layoutlm_2019,
	title = {{LayoutLM}: Pre-training of Text and Layout for Document Image Understanding},
	url = {https://arxiv.org/abs/1912.13318v5},
	doi = {10.1145/3394486.3403172},
	shorttitle = {{LayoutLM}},
	abstract = {Pre-training techniques have been verified successfully in a variety of {NLP} tasks in recent years. Despite the widespread use of pre-training models for {NLP} applications, they almost exclusively focus on text-level manipulation, while neglecting layout and style information that is vital for document image understanding. In this paper, we propose the {\textbackslash}textbf\{{LayoutLM}\} to jointly model interactions between text and layout information across scanned document images, which is beneficial for a great number of real-world document image understanding tasks such as information extraction from scanned documents. Furthermore, we also leverage image features to incorporate words' visual information into {LayoutLM}. To the best of our knowledge, this is the first time that text and layout are jointly learned in a single framework for document-level pre-training. It achieves new state-of-the-art results in several downstream tasks, including form understanding (from 70.72 to 79.27), receipt understanding (from 94.02 to 95.24) and document image classification (from 93.07 to 94.42). The code and pre-trained {LayoutLM} models are publicly available at {\textbackslash}url\{https://aka.ms/layoutlm\}.},
	author = {Xu, Yiheng and Li, Minghao and Cui, Lei and Huang, Shaohan and Wei, Furu and Zhou, Ming},
	urldate = {2021-11-11},
	date = {2019-12-31},
	langid = {english},
	file = {Full Text PDF:/Users/assa/Zotero/storage/PZMH9FTY/Xu et al. - 2019 - LayoutLM Pre-training of Text and Layout for Docu.pdf:application/pdf;Snapshot:/Users/assa/Zotero/storage/29IRKULB/1912.html:text/html},
}

@article{xu_layoutxlm_2021,
	title = {{LayoutXLM}: Multimodal Pre-training for Multilingual Visually-rich Document Understanding},
	url = {http://arxiv.org/abs/2104.08836},
	shorttitle = {{LayoutXLM}},
	abstract = {Multimodal pre-training with text, layout, and image has achieved {SOTA} performance for visually-rich document understanding tasks recently, which demonstrates the great potential for joint learning across different modalities. In this paper, we present {LayoutXLM}, a multimodal pre-trained model for multilingual document understanding, which aims to bridge the language barriers for visually-rich document understanding. To accurately evaluate {LayoutXLM}, we also introduce a multilingual form understanding benchmark dataset named {XFUND}, which includes form understanding samples in 7 languages (Chinese, Japanese, Spanish, French, Italian, German, Portuguese), and key-value pairs are manually labeled for each language. Experiment results show that the {LayoutXLM} model has signiﬁcantly outperformed the existing {SOTA} crosslingual pre-trained models on the {XFUND} dataset. The pre-trained {LayoutXLM} model and the {XFUND} dataset are publicly available at https://aka.ms/layoutxlm.},
	journaltitle = {{arXiv}:2104.08836 [cs]},
	author = {Xu, Yiheng and Lv, Tengchao and Cui, Lei and Wang, Guoxin and Lu, Yijuan and Florencio, Dinei and Zhang, Cha and Wei, Furu},
	urldate = {2021-11-22},
	date = {2021-09-09},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2104.08836},
	keywords = {Computer Science - Computation and Language},
	file = {Xu et al. - 2021 - LayoutXLM Multimodal Pre-training for Multilingua.pdf:/Users/assa/Zotero/storage/KNKRS6PR/Xu et al. - 2021 - LayoutXLM Multimodal Pre-training for Multilingua.pdf:application/pdf},
}

@article{xu_layoutlmv2_2021,
	title = {{LayoutLMv}2: Multi-modal Pre-training for Visually-Rich Document Understanding},
	url = {http://arxiv.org/abs/2012.14740},
	shorttitle = {{LayoutLMv}2},
	abstract = {Pre-training of text and layout has proved effective in a variety of visually-rich document understanding tasks due to its effective model architecture and the advantage of large-scale unlabeled scanned/digital-born documents. In this paper, we present {\textbackslash}textbf\{{LayoutLMv}2\} by pre-training text, layout and image in a multi-modal framework, where new model architectures and pre-training tasks are leveraged. Specifically, {LayoutLMv}2 not only uses the existing masked visual-language modeling task but also the new text-image alignment and text-image matching tasks in the pre-training stage, where cross-modality interaction is better learned. Meanwhile, it also integrates a spatial-aware self-attention mechanism into the Transformer architecture, so that the model can fully understand the relative positional relationship among different text blocks. Experiment results show that {LayoutLMv}2 outperforms strong baselines and achieves new state-of-the-art results on a wide variety of downstream visually-rich document understanding tasks, including {FUNSD} (0.7895 -{\textgreater} 0.8420), {CORD} (0.9493 -{\textgreater} 0.9601), {SROIE} (0.9524 -{\textgreater} 0.9781), Kleister-{NDA} (0.834 -{\textgreater} 0.852), {RVL}-{CDIP} (0.9443 -{\textgreater} 0.9564), and {DocVQA} (0.7295 -{\textgreater} 0.8672). The pre-trained {LayoutLMv}2 model is publicly available at https://aka.ms/layoutlmv2.},
	journaltitle = {{arXiv}:2012.14740 [cs]},
	author = {Xu, Yang and Xu, Yiheng and Lv, Tengchao and Cui, Lei and Wei, Furu and Wang, Guoxin and Lu, Yijuan and Florencio, Dinei and Zhang, Cha and Che, Wanxiang and Zhang, Min and Zhou, Lidong},
	urldate = {2021-12-06},
	date = {2021-05-11},
	eprinttype = {arxiv},
	eprint = {2012.14740},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/assa/Zotero/storage/NWNLEB4P/Xu et al. - 2021 - LayoutLMv2 Multi-modal Pre-training for Visually-.pdf:application/pdf;arXiv.org Snapshot:/Users/assa/Zotero/storage/GNYP7JQE/2012.html:text/html},
}

@inproceedings{huang_mask_2019,
	location = {Long Beach, {CA}, {USA}},
	title = {Mask Scoring R-{CNN}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953609/},
	doi = {10.1109/CVPR.2019.00657},
	abstract = {Letting a deep network be aware of the quality of its own predictions is an interesting yet important problem. In the task of instance segmentation, the conﬁdence of instance classiﬁcation is used as mask quality score in most instance segmentation frameworks. However, the mask quality, quantiﬁed as the {IoU} between the instance mask and its ground truth, is usually not well correlated with classiﬁcation score. In this paper, we study this problem and propose Mask Scoring R-{CNN} which contains a network block to learn the quality of the predicted instance masks. The proposed network block takes the instance feature and the corresponding predicted mask together to regress the mask {IoU}. The mask scoring strategy calibrates the misalignment between mask quality and mask score, and improves instance segmentation performance by prioritizing more accurate mask predictions during {COCO} {AP} evaluation. By extensive evaluations on the {COCO} dataset, Mask Scoring R-{CNN} brings consistent and noticeable gain with different models and outperforms the state-of-the-art Mask {RCNN}. We hope our simple and effective approach will provide a new direction for improving instance segmentation. The source code of our method is available at https:// github.com/zjhuang22/maskscoring\_rcnn.},
	eventtitle = {2019 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {6402--6411},
	booktitle = {2019 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Huang, Zhaojin and Huang, Lichao and Gong, Yongchao and Huang, Chang and Wang, Xinggang},
	urldate = {2021-12-16},
	date = {2019-06},
	langid = {english},
	file = {Huang et al. - 2019 - Mask Scoring R-CNN.pdf:/Users/assa/Zotero/storage/9SA55N9D/Huang et al. - 2019 - Mask Scoring R-CNN.pdf:application/pdf},
}

@article{li_rethinking_2020,
	title = {{RETHINKING} {THE} {HYPERPARAMETERS} {FOR} {FINE}-{TUNING}},
	abstract = {Fine-tuning from pre-trained {ImageNet} models has become the de-facto standard for various computer vision tasks. Current practices for ﬁne-tuning typically involve selecting an ad-hoc choice of hyperparameters and keeping them ﬁxed to values normally used for training from scratch. This paper re-examines several common practices of setting hyperparameters for ﬁne-tuning. Our ﬁndings are based on extensive empirical evaluation for ﬁne-tuning on various transfer learning benchmarks. (1) While prior works have thoroughly investigated learning rate and batch size, momentum for ﬁne-tuning is a relatively unexplored parameter. We ﬁnd that the value of momentum also affects ﬁne-tuning performance and connect it with previous theoretical ﬁndings. (2) Optimal hyperparameters for ﬁne-tuning, in particular, the effective learning rate, are not only dataset dependent but also sensitive to the similarity between the source domain and target domain. This is in contrast to hyperparameters for training from scratch. (3) Reference-based regularization that keeps models close to the initial model does not necessarily apply for “dissimilar” datasets. Our ﬁndings challenge common practices of ﬁnetuning and encourages deep learning practitioners to rethink the hyperparameters for ﬁne-tuning.},
	pages = {20},
	author = {Li, Hao and Chaudhari, Pratik and Yang, Hao and Lam, Michael and Ravichandran, Avinash and Bhotika, Rahul and Soatto, Stefano},
	date = {2020},
	langid = {english},
	file = {Li et al. - 2020 - RETHINKING THE HYPERPARAMETERS FOR FINE-TUNING.pdf:/Users/assa/Zotero/storage/67XZZ9LU/Li et al. - 2020 - RETHINKING THE HYPERPARAMETERS FOR FINE-TUNING.pdf:application/pdf},
}

@article{ronneberger_u-net_2015,
	title = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
	url = {http://arxiv.org/abs/1505.04597},
	shorttitle = {U-Net},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the {ISBI} challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and {DIC}) we won the {ISBI} cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent {GPU}. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	journaltitle = {{arXiv}:1505.04597 [cs]},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	urldate = {2021-12-23},
	date = {2015-05-18},
	eprinttype = {arxiv},
	eprint = {1505.04597},
	note = {version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/assa/Zotero/storage/KHEHTKEC/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf:application/pdf;arXiv.org Snapshot:/Users/assa/Zotero/storage/2VBZPYIF/1505.html:text/html},
}

@article{lin_microsoft_2014,
	title = {Microsoft {COCO}: Common Objects in Context},
	url = {https://arxiv.org/abs/1405.0312v3},
	shorttitle = {Microsoft {COCO}},
	abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to {PASCAL}, {ImageNet}, and {SUN}. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
	urldate = {2021-12-23},
	date = {2014-05-01},
	langid = {english},
	file = {Snapshot:/Users/assa/Zotero/storage/REPM8WN5/1405.html:text/html;Full Text PDF:/Users/assa/Zotero/storage/8G8JNH2U/Lin et al. - 2014 - Microsoft COCO Common Objects in Context.pdf:application/pdf},
}

@article{chen_mmdetection_2019,
	title = {{MMDetection}: Open {MMLab} Detection Toolbox and Benchmark},
	url = {http://arxiv.org/abs/1906.07155},
	shorttitle = {{MMDetection}},
	abstract = {We present {MMDetection}, an object detection toolbox that contains a rich set of object detection and instance segmentation methods as well as related components and modules. The toolbox started from a codebase of {MMDet} team who won the detection track of {COCO} Challenge 2018. It gradually evolves into a unified platform that covers many popular detection methods and contemporary modules. It not only includes training and inference codes, but also provides weights for more than 200 network models. We believe this toolbox is by far the most complete detection toolbox. In this paper, we introduce the various features of this toolbox. In addition, we also conduct a benchmarking study on different methods, components, and their hyper-parameters. We wish that the toolbox and benchmark could serve the growing research community by providing a flexible toolkit to reimplement existing methods and develop their own new detectors. Code and models are available at https://github.com/open-mmlab/mmdetection. The project is under active development and we will keep this document updated.},
	journaltitle = {{arXiv}:1906.07155 [cs, eess]},
	author = {Chen, Kai and Wang, Jiaqi and Pang, Jiangmiao and Cao, Yuhang and Xiong, Yu and Li, Xiaoxiao and Sun, Shuyang and Feng, Wansen and Liu, Ziwei and Xu, Jiarui and Zhang, Zheng and Cheng, Dazhi and Zhu, Chenchen and Cheng, Tianheng and Zhao, Qijie and Li, Buyu and Lu, Xin and Zhu, Rui and Wu, Yue and Dai, Jifeng and Wang, Jingdong and Shi, Jianping and Ouyang, Wanli and Loy, Chen Change and Lin, Dahua},
	urldate = {2021-12-23},
	date = {2019-06-17},
	eprinttype = {arxiv},
	eprint = {1906.07155},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/Users/assa/Zotero/storage/NWR98FP3/Chen et al. - 2019 - MMDetection Open MMLab Detection Toolbox and Benc.pdf:application/pdf;arXiv.org Snapshot:/Users/assa/Zotero/storage/L5EIHR97/1906.html:text/html},
}

@article{hafiz_survey_2020,
	title = {A survey on instance segmentation: state of the art},
	volume = {9},
	issn = {2192-6611, 2192-662X},
	url = {https://link.springer.com/10.1007/s13735-020-00195-x},
	doi = {10.1007/s13735-020-00195-x},
	shorttitle = {A survey on instance segmentation},
	abstract = {Object detection or localization is an incremental step in progression from coarse to fine digital image inference. It not only provides the classes of the image objects, but also provides the location of the image objects which have been classified. The location is given in the form of bounding boxes or centroids. Semantic segmentation gives ﬁne inference by predicting labels for every pixel in the input image. Each pixel is labelled according to the object class within which it is enclosed. Furthering this evolution, instance segmentation gives different labels for separate instances of objects belonging to the same class. Hence, instance segmentation may be defined as the technique of simultaneously solving the problem of object detection as well as that of semantic segmentation. In this survey paper on instance segmentation- its background, issues, techniques, evolution, popular datasets, related work up to the state of the art and future scope have been discussed. The paper provides valuable information for those who want to do research in the field of instance segmentation.},
	pages = {171--189},
	number = {3},
	journaltitle = {International Journal of Multimedia Information Retrieval},
	shortjournal = {Int J Multimed Info Retr},
	author = {Hafiz, Abdul Mueed and Bhat, Ghulam Mohiuddin},
	urldate = {2021-12-23},
	date = {2020-09},
	langid = {english},
	file = {Hafiz et Bhat - 2020 - A survey on instance segmentation state of the ar.pdf:/Users/assa/Zotero/storage/XYAUMQKA/Hafiz et Bhat - 2020 - A survey on instance segmentation state of the ar.pdf:application/pdf},
}

@article{lin_feature_2017,
	title = {Feature Pyramid Networks for Object Detection},
	url = {http://arxiv.org/abs/1612.03144},
	abstract = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network ({FPN}), shows significant improvement as a generic feature extractor in several applications. Using {FPN} in a basic Faster R-{CNN} system, our method achieves state-of-the-art single-model results on the {COCO} detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the {COCO} 2016 challenge winners. In addition, our method can run at 5 {FPS} on a {GPU} and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
	journaltitle = {{arXiv}:1612.03144 [cs]},
	author = {Lin, Tsung-Yi and Dollár, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
	urldate = {2021-12-23},
	date = {2017-04-19},
	eprinttype = {arxiv},
	eprint = {1612.03144},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/assa/Zotero/storage/ETTEKS7N/Lin et al. - 2017 - Feature Pyramid Networks for Object Detection.pdf:application/pdf;arXiv.org Snapshot:/Users/assa/Zotero/storage/PUCDCH3E/1612.html:text/html},
}

@article{liu_roberta_2019,
	title = {{RoBERTa}: A Robustly Optimized {BERT} Pretraining Approach},
	url = {http://arxiv.org/abs/1907.11692},
	shorttitle = {{RoBERTa}},
	abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of {BERT} pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that {BERT} was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on {GLUE}, {RACE} and {SQuAD}. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
	journaltitle = {{arXiv}:1907.11692 [cs]},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	urldate = {2021-12-23},
	date = {2019-07-26},
	eprinttype = {arxiv},
	eprint = {1907.11692},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/assa/Zotero/storage/9KXCV2X5/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf:application/pdf;arXiv.org Snapshot:/Users/assa/Zotero/storage/KH9QQYI8/1907.html:text/html},
}

@article{devlin_bert_2019,
	title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	url = {http://arxiv.org/abs/1810.04805},
	shorttitle = {{BERT}},
	abstract = {We introduce a new language representation model called {BERT}, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, {BERT} is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained {BERT} model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. {BERT} is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the {GLUE} score to 80.5\% (7.7\% point absolute improvement), {MultiNLI} accuracy to 86.7\% (4.6\% absolute improvement), {SQuAD} v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and {SQuAD} v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	journaltitle = {{arXiv}:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	urldate = {2021-12-23},
	date = {2019-05-24},
	eprinttype = {arxiv},
	eprint = {1810.04805},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/assa/Zotero/storage/DYXWK5YD/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf;arXiv.org Snapshot:/Users/assa/Zotero/storage/HEWGSD6G/1810.html:text/html},
}

@article{conneau_unsupervised_2020,
	title = {Unsupervised Cross-lingual Representation Learning at Scale},
	url = {http://arxiv.org/abs/1911.02116},
	abstract = {This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered {CommonCrawl} data. Our model, dubbed {XLM}-R, significantly outperforms multilingual {BERT} ({mBERT}) on a variety of cross-lingual benchmarks, including +14.6\% average accuracy on {XNLI}, +13\% average F1 score on {MLQA}, and +2.4\% F1 score on {NER}. {XLM}-R performs particularly well on low-resource languages, improving 15.7\% in {XNLI} accuracy for Swahili and 11.4\% for Urdu over previous {XLM} models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; {XLM}-R is very competitive with strong monolingual models on the {GLUE} and {XNLI} benchmarks. We will make our code, data and models publicly available.},
	journaltitle = {{arXiv}:1911.02116 [cs]},
	author = {Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzmán, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
	urldate = {2021-12-23},
	date = {2020-04-07},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1911.02116},
	keywords = {Computer Science - Computation and Language},
	file = {Conneau et al. - 2020 - Unsupervised Cross-lingual Representation Learning.pdf:/Users/assa/Zotero/storage/N4ADXZRQ/Conneau et al. - 2020 - Unsupervised Cross-lingual Representation Learning.pdf:application/pdf},
}

@article{he_deep_2015,
	title = {Deep Residual Learning for Image Recognition},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the {ImageNet} dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than {VGG} nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the {ImageNet} test set. This result won the 1st place on the {ILSVRC} 2015 classification task. We also present analysis on {CIFAR}-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the {COCO} object detection dataset. Deep residual nets are foundations of our submissions to {ILSVRC} \& {COCO} 2015 competitions, where we also won the 1st places on the tasks of {ImageNet} detection, {ImageNet} localization, {COCO} detection, and {COCO} segmentation.},
	journaltitle = {{arXiv}:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	urldate = {2021-12-28},
	date = {2015-12-10},
	eprinttype = {arxiv},
	eprint = {1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/assa/Zotero/storage/R5XGCP4Y/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv.org Snapshot:/Users/assa/Zotero/storage/MXGAB3DE/1512.html:text/html},
}

@inproceedings{deng_imagenet_2009,
	title = {{ImageNet}: A large-scale hierarchical image database},
	doi = {10.1109/CVPR.2009.5206848},
	shorttitle = {{ImageNet}},
	abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “{ImageNet}”, a large-scale ontology of images built upon the backbone of the {WordNet} structure. {ImageNet} aims to populate the majority of the 80,000 synsets of {WordNet} with an average of 500–1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of {WordNet}. This paper offers a detailed analysis of {ImageNet} in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that {ImageNet} is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of {ImageNet} through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of {ImageNet} can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
	eventtitle = {2009 {IEEE} Conference on Computer Vision and Pattern Recognition},
	pages = {248--255},
	booktitle = {2009 {IEEE} Conference on Computer Vision and Pattern Recognition},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	date = {2009-06},
	note = {{ISSN}: 1063-6919},
	keywords = {Explosions, Image databases, Image retrieval, Information retrieval, Internet, Large-scale systems, Multimedia databases, Ontologies, Robustness, Spine},
	file = {IEEE Xplore Full Text PDF:/Users/assa/Zotero/storage/2E2MWABT/Deng et al. - 2009 - ImageNet A large-scale hierarchical image databas.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/assa/Zotero/storage/NK7238XR/5206848.html:text/html},
}

@article{wolf_huggingfaces_2020,
	title = {{HuggingFace}'s Transformers: State-of-the-art Natural Language Processing},
	url = {http://arxiv.org/abs/1910.03771},
	shorttitle = {{HuggingFace}'s Transformers},
	abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. {\textbackslash}textit\{Transformers\} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified {API}. Backing this library is a curated collection of pretrained models made by and available for the community. {\textbackslash}textit\{Transformers\} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at {\textbackslash}url\{https://github.com/huggingface/transformers\}.},
	journaltitle = {{arXiv}:1910.03771 [cs]},
	author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Rémi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Scao, Teven Le and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.},
	urldate = {2021-12-28},
	date = {2020-07-13},
	eprinttype = {arxiv},
	eprint = {1910.03771},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/assa/Zotero/storage/I4FHLQLV/Wolf et al. - 2020 - HuggingFace's Transformers State-of-the-art Natur.pdf:application/pdf;arXiv.org Snapshot:/Users/assa/Zotero/storage/5AZP9MCH/1910.html:text/html},
}

@article{ren_faster_2016,
	title = {Faster R-{CNN}: Towards Real-Time Object Detection with Region Proposal Networks},
	url = {http://arxiv.org/abs/1506.01497},
	shorttitle = {Faster R-{CNN}},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like {SPPnet} and Fast R-{CNN} have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network ({RPN}) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An {RPN} is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The {RPN} is trained end-to-end to generate high-quality region proposals, which are used by Fast R-{CNN} for detection. We further merge {RPN} and Fast R-{CNN} into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the {RPN} component tells the unified network where to look. For the very deep {VGG}-16 model, our detection system has a frame rate of 5fps (including all steps) on a {GPU}, while achieving state-of-the-art object detection accuracy on {PASCAL} {VOC} 2007, 2012, and {MS} {COCO} datasets with only 300 proposals per image. In {ILSVRC} and {COCO} 2015 competitions, Faster R-{CNN} and {RPN} are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
	journaltitle = {{arXiv}:1506.01497 [cs]},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	urldate = {2021-12-28},
	date = {2016-01-06},
	eprinttype = {arxiv},
	eprint = {1506.01497},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/assa/Zotero/storage/DBZ2QD5Y/Ren et al. - 2016 - Faster R-CNN Towards Real-Time Object Detection w.pdf:application/pdf;arXiv.org Snapshot:/Users/assa/Zotero/storage/DCFAWZP4/1506.html:text/html},
}

@inproceedings{gao_icdar_2019,
	title = {{ICDAR} 2019 Competition on Table Detection and Recognition ({cTDaR})},
	doi = {10.1109/ICDAR.2019.00243},
	abstract = {The {cTDaR} competition aims at benchmarking state-of-the-art table detection ({TRACK} A) and table recognition ({TRACK} B) methods. In particular, we wish to investigate and compare general methods that can reliably and robustly identify the table regions within a document image on the one hand, and the table structure on the other hand. Due to the presence of hand-drawn tables and handwritten text, the methods must be robust against various noise conditions, interfering annotations, and variations of the tables. Two new challenging datasets were created to test the behaviour of state-of-the-art table detection and recognition systems on real world data. One dataset consists of modern documents, while the other consists of archival documents with presence of hand-drawn tables and handwritten text. The evaluation scheme is adapted from the {ICDAR} 2013 Table competition. We received results of Track A from 11 teams and results of Track B from 2 teams. Results for Track A are very good for the top participants. The winner and his runner-up are very close while using very different approaches. Track B was more challenging and only one participant was able to produce good results.},
	eventtitle = {2019 International Conference on Document Analysis and Recognition ({ICDAR})},
	pages = {1510--1515},
	booktitle = {2019 International Conference on Document Analysis and Recognition ({ICDAR})},
	author = {Gao, Liangcai and Huang, Yilun and Déjean, Hervé and Meunier, Jean-Luc and Yan, Qinqin and Fang, Yu and Kleber, Florian and Lang, Eva},
	date = {2019-09},
	note = {{ISSN}: 2379-2140},
	keywords = {Image segmentation, Training, Atmospheric modeling, Computational modeling, Data models, Image edge detection, table detection, table recognition, modern tables, archival documents, {XML}},
	file = {IEEE Xplore Full Text PDF:/Users/assa/Zotero/storage/DAYWH75D/Gao et al. - 2019 - ICDAR 2019 Competition on Table Detection and Reco.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/assa/Zotero/storage/86TXNDCZ/8978120.html:text/html},
}

@thesis{seguin_making_2018,
	location = {Lausanne},
	title = {Making large art historical photo archives searchable},
	abstract = {In recent years, museums, archives and other cultural institutions have initiated important programs to digitize their collections. Millions of artefacts (paintings, engravings, drawings, ancient photographs) are now represented in digital photographic format. Furthermore, through progress in standardization, a growing portion of these images are now available online, in an easily accessible manner. This thesis studies how such large-scale art history collection can be made searchable using new deep learning approaches for processing and comparing images. It takes as a case study the processing of the photo archive of the Foundation Giorgio Cini, where more than 300'000 images have been digitized. We demonstrate how a generic processing pipeline can reliably extract the visual and textual content of scanned images, opening up ways to efficiently digitize large photo-collections. Then, by leveraging an annotated graph of visual connections, a metric is learnt that allows clustering and searching through artwork reproductions independently of their medium, effectively solving a difficult problem of cross-domain image search. Finally, the thesis studies how a complex Web Interface allows users to perform different searches based on this metric. We also evaluate the process by which users can annotate elements of interest during their navigation to be added to the database, allowing the system to be trained further and give better results. By documenting a complete approach on how to go from a physical photo-archive to a state-of-the-art navigation system, this thesis paves the way for a global search engine across the world's photo archives},
	pagetotal = {169},
	institution = {{EPFL}},
	type = {phdthesis},
	author = {Seguin, Benoît Laurent Auguste},
	date = {2018},
	doi = {10.5075/epfl-thesis-8857},
	keywords = {deep learning, computer vision, archives, art history, digitization, large image collections, visual search},
}

@article{simonyan_very_2015,
	title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our {ImageNet} Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing {ConvNet} models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	journaltitle = {{arXiv}:1409.1556 [cs]},
	author = {Simonyan, Karen and Zisserman, Andrew},
	urldate = {2021-12-30},
	date = {2015-04-10},
	eprinttype = {arxiv},
	eprint = {1409.1556},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/assa/Zotero/storage/T9GEM646/Simonyan et Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf:application/pdf;arXiv.org Snapshot:/Users/assa/Zotero/storage/DTAGEKRI/1409.html:text/html},
}

@article{szegedy_inception-v4_2016,
	title = {Inception-v4, Inception-{ResNet} and the Impact of Residual Connections on Learning},
	url = {http://arxiv.org/abs/1602.07261},
	abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 {ILSVRC} challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the {ILSVRC} 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the {ImageNet} classification ({CLS}) challenge},
	journaltitle = {{arXiv}:1602.07261 [cs]},
	author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alex},
	urldate = {2021-12-30},
	date = {2016-08-23},
	eprinttype = {arxiv},
	eprint = {1602.07261},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/assa/Zotero/storage/VPPA8YB5/Szegedy et al. - 2016 - Inception-v4, Inception-ResNet and the Impact of R.pdf:application/pdf;arXiv.org Snapshot:/Users/assa/Zotero/storage/KYAC3927/1602.html:text/html},
}

@inproceedings{dutta_via_2019,
	location = {Nice France},
	title = {The {VIA} Annotation Software for Images, Audio and Video},
	isbn = {978-1-4503-6889-6},
	url = {https://dl.acm.org/doi/10.1145/3343031.3350535},
	doi = {10.1145/3343031.3350535},
	eventtitle = {{MM} '19: The 27th {ACM} International Conference on Multimedia},
	pages = {2276--2279},
	booktitle = {Proceedings of the 27th {ACM} International Conference on Multimedia},
	publisher = {{ACM}},
	author = {Dutta, Abhishek and Zisserman, Andrew},
	urldate = {2022-01-02},
	date = {2019-10-15},
	langid = {english},
	file = {Version soumise:/Users/assa/Zotero/storage/7U6FUGLH/Dutta et Zisserman - 2019 - The VIA Annotation Software for Images, Audio and .pdf:application/pdf},
}

@inproceedings{almutairi_instance_2019,
	title = {Instance Segmentation of Newspaper Elements Using Mask R-{CNN}},
	doi = {10.1109/ICMLA.2019.00223},
	abstract = {Newspaper digitization has gained wide interest around the world. Archives of digitized newspapers contain a wealth of information that spans decades. To extract this abundance of information, optical character recognition ({OCR}) techniques are used. However, as a first step, the newspaper pages should be logically deconstructed into articles to gain meaningful knowledge. This is difficult due to the complex layout of newspapers and the various styles, shapes, and languages of newspaper articles. Newspaper pages also contain other elements besides articles, such as advertisements that come in multiple shapes and forms, and top headers that contain information about the newspaper's issue and page. Therefore, it is important to detect these elements before information extraction begins. In this paper, we present a deep learning solution for the problem of newspaper page semantic segmentation of the main newspaper elements (articles, advertisements, and page headers). We employed the instance segmentation method mask R-{CNN} mask\_rcnn to create a language-agnostic model that logically deconstructs a newspaper page raw image into its main elements based only on its visual features. We show the results of experiments that display the accuracy and robustness of our model.},
	eventtitle = {2019 18th {IEEE} International Conference On Machine Learning And Applications ({ICMLA})},
	pages = {1371--1375},
	booktitle = {2019 18th {IEEE} International Conference On Machine Learning And Applications ({ICMLA})},
	author = {Almutairi, Abdullah and Almashan, Meshal},
	date = {2019-12},
	keywords = {Image segmentation, Layout, Training, Feature extraction, Machine learning, newspaper article segmentation, semantic segmentation, deep learning, mask R-{CNN}, Shape, Training data},
	file = {IEEE Xplore Abstract Record:/Users/assa/Zotero/storage/SWU53HW8/8999273.html:text/html},
}

@article{hashmi_current_2021,
	title = {Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks},
	url = {http://arxiv.org/abs/2104.14272},
	abstract = {The ﬁrst phase of table recognition is to detect the tabular area in a document. Subsequently, the tabular structures are recognized in the second phase in order to extract information from the respective cells. Table detection and structural recognition are pivotal problems in the domain of table understanding. However, table analysis is a perplexing task due to the colossal amount of diversity and asymmetry in tables. Therefore, it is an active area of research in document image analysis. Recent advances in the computing capabilities of graphical processing units have enabled the deep neural networks to outperform traditional state-of-the-art machine learning methods. Table understanding has substantially beneﬁted from the recent breakthroughs in deep neural networks. However, there has not been a consolidated description of the deep learning methods for table detection and table structure recognition. This review paper provides a thorough analysis of the modern methodologies that utilize deep neural networks. This work provided a thorough understanding of the current state-of-the-art and related challenges of table understanding in document images. Furthermore, the leading datasets and their intricacies have been elaborated along with the quantitative results. Moreover, a brief overview is given regarding the promising directions that can serve as a guide to further improve table analysis in document images.},
	journaltitle = {{arXiv}:2104.14272 [cs]},
	author = {Hashmi, Khurram Azeem and Liwicki, Marcus and Stricker, Didier and Afzal, Muhammad Adnan and Afzal, Muhammad Ahtsham and Afzal, Muhammad Zeshan},
	urldate = {2022-01-05},
	date = {2021-05-08},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2104.14272},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Hashmi et al. - 2021 - Current Status and Performance Analysis of Table R.pdf:/Users/assa/Zotero/storage/QC7PY777/Hashmi et al. - 2021 - Current Status and Performance Analysis of Table R.pdf:application/pdf},
}

@article{lee_newspaper_2020,
	title = {The Newspaper Navigator Dataset: Extracting And Analyzing Visual Content from 16 Million Historic Newspaper Pages in Chronicling America},
	url = {http://arxiv.org/abs/2005.01583},
	shorttitle = {The Newspaper Navigator Dataset},
	abstract = {Chronicling America is a product of the National Digital Newspaper Program, a partnership between the Library of Congress and the National Endowment for the Humanities to digitize historic newspapers. Over 16 million pages of historic American newspapers have been digitized for Chronicling America to date, complete with high-resolution images and machine-readable {METS}/{ALTO} {OCR}. Of considerable interest to Chronicling America users is a semantified corpus, complete with extracted visual content and headlines. To accomplish this, we introduce a visual content recognition model trained on bounding box annotations of photographs, illustrations, maps, comics, and editorial cartoons collected as part of the Library of Congress's Beyond Words crowdsourcing initiative and augmented with additional annotations including those of headlines and advertisements. We describe our pipeline that utilizes this deep learning model to extract 7 classes of visual content: headlines, photographs, illustrations, maps, comics, editorial cartoons, and advertisements, complete with textual content such as captions derived from the {METS}/{ALTO} {OCR}, as well as image embeddings for fast image similarity querying. We report the results of running the pipeline on 16.3 million pages from the Chronicling America corpus and describe the resulting Newspaper Navigator dataset, the largest dataset of extracted visual content from historic newspapers ever produced. The Newspaper Navigator dataset, finetuned visual content recognition model, and all source code are placed in the public domain for unrestricted re-use.},
	journaltitle = {{arXiv}:2005.01583 [cs]},
	author = {Lee, Benjamin Charles Germain and Mears, Jaime and Jakeway, Eileen and Ferriter, Meghan and Adams, Chris and Yarasavage, Nathan and Thomas, Deborah and Zwaard, Kate and Weld, Daniel S.},
	urldate = {2022-01-06},
	date = {2020-05-04},
	eprinttype = {arxiv},
	eprint = {2005.01583},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:/Users/assa/Zotero/storage/YIMDGWJP/Lee et al. - 2020 - The Newspaper Navigator Dataset Extracting And An.pdf:application/pdf;arXiv.org Snapshot:/Users/assa/Zotero/storage/G9XZGMAD/2005.html:text/html},
}

@inproceedings{casado-garcia_benefits_2020,
	location = {Cham},
	title = {The Benefits of Close-Domain Fine-Tuning for Table Detection in Document Images},
	isbn = {978-3-030-57058-3},
	doi = {10.1007/978-3-030-57058-3_15},
	series = {Lecture Notes in Computer Science},
	abstract = {A correct localisation of tables in a document is instrumental for determining their structure and extracting their contents; therefore, table detection is a key step in table understanding. Nowadays, the most successful methods for table detection in document images employ deep learning algorithms; and, particularly, a technique known as fine-tuning. In this context, such a technique exports the knowledge acquired to detect objects in natural images to detect tables in document images. However, there is only a vague relation between natural and document images, and fine-tuning works better when there is a close relation between the source and target task. In this paper, we show that it is more beneficial to employ fine-tuning from a closer domain. To this aim, we train different object detection algorithms (namely, Mask R-{CNN}, {RetinaNet}, {SSD} and {YOLO}) using the {TableBank} dataset (a dataset of images of academic documents designed for table detection and recognition), and fine-tune them for several heterogeneous table detection datasets. Using this approach, we considerably improve the accuracy of the detection models fine-tuned from natural images (in mean a 17\%, and, in the best case, up to a 60\%).},
	pages = {199--215},
	booktitle = {Document Analysis Systems},
	publisher = {Springer International Publishing},
	author = {Casado-García, Ángela and Domínguez, César and Heras, Jónathan and Mata, Eloy and Pascual, Vico},
	editor = {Bai, Xiang and Karatzas, Dimosthenis and Lopresti, Daniel},
	date = {2020},
	langid = {english},
	keywords = {Deep learning, Fine-tuning, Table detection, Transfer learning},
	file = {Springer Full Text PDF:/Users/assa/Zotero/storage/L5WB37FV/Casado-García et al. - 2020 - The Benefits of Close-Domain Fine-Tuning for Table.pdf:application/pdf},
}

@inproceedings{huang_yolo-based_2019,
	title = {A {YOLO}-Based Table Detection Method},
	doi = {10.1109/ICDAR.2019.00135},
	abstract = {Due to various table layouts and styles, table detection is always a difficult task in the field of document analysis. Inspired by the great progress of deep learning based methods on object detection, in this paper, we present a {YOLO}-based method for this task. Considering the large difference between document objects and natural objects, we introduce some adaptive adjustments to {YOLOv}3, including an anchor optimization strategy and two post processing methods. For anchor optimization, we use k-means clustering to find anchors which are more suitable for tables rather than natural objects and make it easier for our model to find exact positions of tables. In post-processing process, the extra whitespaces and noisy page objects (e.g. page headers, page footers) are removed from the predicted results, so that our model can get more accurate table margins and higher {IoU} scores. The proposed method is evaluated on two datasets from {ICDAR} 2013 Table Competition and {ICDAR} 2017 Page Object Detection ({POD}) Competition and achieves state-of-the-art performance.},
	eventtitle = {2019 International Conference on Document Analysis and Recognition ({ICDAR})},
	pages = {813--818},
	booktitle = {2019 International Conference on Document Analysis and Recognition ({ICDAR})},
	author = {Huang, Yilun and Yan, Qinqin and Li, Yibo and Chen, Yifan and Wang, Xiong and Gao, Liangcai and Tang, Zhi},
	date = {2019-09},
	note = {{ISSN}: 2379-2140},
	keywords = {deep learning, Layout, Task analysis, Deep learning, Feature extraction, Adaptation models, document analysis, Optimization, Portable document format, post processing, table detection},
	file = {IEEE Xplore Full Text PDF:/Users/assa/Zotero/storage/B6G7VH3A/Huang et al. - 2019 - A YOLO-Based Table Detection Method.pdf:application/pdf},
}

@inproceedings{zhong_publaynet_2019,
	title = {{PubLayNet}: Largest Dataset Ever for Document Layout Analysis},
	doi = {10.1109/ICDAR.2019.00166},
	shorttitle = {{PubLayNet}},
	abstract = {Recognizing the layout of unstructured digital documents is an important step when parsing the documents into structured machine-readable format for downstream applications. Deep neural networks that are developed for computer vision have been proven to be an effective method to analyze layout of document images. However, document layout datasets that are currently publicly available are several magnitudes smaller than established computing vision datasets. Models have to be trained by transfer learning from a base model that is pre-trained on a traditional computer vision dataset. In this paper, we develop the {PubLayNet} dataset for document layout analysis by automatically matching the {XML} representations and the content of over 1 million {PDF} articles that are publicly available on {PubMed} Central. The size of the dataset is comparable to established computer vision datasets, containing over 360 thousand document images, where typical document layout elements are annotated. The experiments demonstrate that deep neural networks trained on {PubLayNet} accurately recognize the layout of scientific articles. The pre-trained models are also a more effective base mode for transfer learning on a different document domain. We release the dataset (https://github.com/ibm-aur-nlp/{PubLayNet}) to support development and evaluation of more advanced models for document layout analysis.},
	eventtitle = {2019 International Conference on Document Analysis and Recognition ({ICDAR})},
	pages = {1015--1022},
	booktitle = {2019 International Conference on Document Analysis and Recognition ({ICDAR})},
	author = {Zhong, Xu and Tang, Jianbin and Jimeno Yepes, Antonio},
	date = {2019-09},
	note = {{ISSN}: 2379-2140},
	keywords = {Layout, Text analysis, {XML}, Australia, automatic annotation, document layout, deep learning, transfer learning, Proteins, Retina, Veins},
	file = {IEEE Xplore Full Text PDF:/Users/assa/Zotero/storage/U5CIT435/Zhong et al. - 2019 - PubLayNet Largest Dataset Ever for Document Layou.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/assa/Zotero/storage/XXMPHUTQ/8977963.html:text/html},
}

@article{binmakhashen_document_2019,
	title = {Document Layout Analysis: A Comprehensive Survey},
	volume = {52},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3355610},
	doi = {10.1145/3355610},
	shorttitle = {Document Layout Analysis},
	abstract = {Document layout analysis ({DLA}) is a preprocessing step of document understanding systems. It is responsible for detecting and annotating the physical structure of documents. {DLA} has several important applications such as document retrieval, content categorization, text recognition, and the like. The objective of {DLA} is to ease the subsequent analysis/recognition phases by identifying the document-homogeneous blocks and by determining their relationships. The {DLA} pipeline consists of several phases that could vary among {DLA} methods, depending on the documents’ layouts and final analysis objectives. In this regard, a universal {DLA} algorithm that fits all types of document-layouts or that satisfies all analysis objectives has not been developed, yet. In this survey paper, we present a critical study of different document layout analysis techniques. The study highlights the motivational reasons for pursuing {DLA} and discusses comprehensively the different phases of the {DLA} algorithms based on a general framework that is formed as an outcome of reviewing the research in the field. The {DLA} framework consists of preprocessing, layout analysis strategies, post-processing, and performance evaluation phases. Overall, the article delivers an essential baseline for pursuing further research in document layout analysis.},
	pages = {109:1--109:36},
	number = {6},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Binmakhashen, Galal M. and Mahmoud, Sabri A.},
	urldate = {2022-01-06},
	date = {2019-10-16},
	keywords = {document image retrieval, document image understanding, Document segmentation, document structure analysis, layout analysis, physical document structure},
	file = {Full Text PDF:/Users/assa/Zotero/storage/GRYRQ9KU/Binmakhashen et Mahmoud - 2019 - Document Layout Analysis A Comprehensive Survey.pdf:application/pdf},
}

@article{cai_cascade_2021,
	title = {Cascade R-{CNN}: High Quality Object Detection and Instance Segmentation},
	volume = {43},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2019.2956516},
	shorttitle = {Cascade R-{CNN}},
	abstract = {In object detection, the intersection over union ({IoU}) threshold is frequently used to define positives/negatives. The threshold used to train a detector defines its quality. While the commonly used threshold of 0.5 leads to noisy (low-quality) detections, detection performance frequently degrades for larger thresholds. This paradox of high-quality detection has two causes: 1) overfitting, due to vanishing positive samples for large thresholds, and 2) inference-time quality mismatch between detector and test hypotheses. A multi-stage object detection architecture, the Cascade R-{CNN}, composed of a sequence of detectors trained with increasing {IoU} thresholds, is proposed to address these problems. The detectors are trained sequentially, using the output of a detector as training set for the next. This resampling progressively improves hypotheses quality, guaranteeing a positive training set of equivalent size for all detectors and minimizing overfitting. The same cascade is applied at inference, to eliminate quality mismatches between hypotheses and detectors. An implementation of the Cascade R-{CNN} without bells or whistles achieves state-of-the-art performance on the {COCO} dataset, and significantly improves high-quality detection on generic and specific object datasets, including {VOC}, {KITTI}, {CityPerson}, and {WiderFace}. Finally, the Cascade R-{CNN} is generalized to instance segmentation, with nontrivial improvements over the Mask R-{CNN}.},
	pages = {1483--1498},
	number = {5},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Cai, Zhaowei and Vasconcelos, Nuno},
	date = {2021-05},
	note = {Conference Name: {IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Task analysis, Training, Feature extraction, bounding box regression, cascade, Computer architecture, Detectors, high quality, instance segmentation, Object detection, Proposals},
	file = {IEEE Xplore Full Text PDF:/Users/assa/Zotero/storage/GTWMR3UM/Cai et Vasconcelos - 2021 - Cascade R-CNN High Quality Object Detection and I.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/assa/Zotero/storage/Z4XCNRXM/8917599.html:text/html},
}

@article{chen_deeplab_2017,
	title = {{DeepLab}: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected {CRFs}},
	url = {http://arxiv.org/abs/1606.00915},
	shorttitle = {{DeepLab}},
	abstract = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled ﬁlters, or ‘atrous convolution’, as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the ﬁeld of view of ﬁlters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling ({ASPP}) to robustly segment objects at multiple scales. {ASPP} probes an incoming convolutional feature layer with ﬁlters at multiple sampling rates and effective ﬁelds-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from {DCNNs} and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in {DCNNs} achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the ﬁnal {DCNN} layer with a fully connected Conditional Random Field ({CRF}), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed “{DeepLab}” system sets the new state-of-art at the {PASCAL} {VOC}-2012 semantic image segmentation task, reaching 79.7\% {mIOU} in the test set, and advances the results on three other datasets: {PASCAL}-Context, {PASCAL}-Person-Part, and Cityscapes. All of our code is made publicly available online.},
	journaltitle = {{arXiv}:1606.00915 [cs]},
	author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
	urldate = {2022-01-06},
	date = {2017-05-11},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1606.00915},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Chen et al. - 2017 - DeepLab Semantic Image Segmentation with Deep Con.pdf:/Users/assa/Zotero/storage/CX285G32/Chen et al. - 2017 - DeepLab Semantic Image Segmentation with Deep Con.pdf:application/pdf},
}

@article{vaswani_attention_2017,
	title = {Attention Is All You Need},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	journaltitle = {{arXiv}:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2022-01-06},
	date = {2017-12-05},
	eprinttype = {arxiv},
	eprint = {1706.03762},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/assa/Zotero/storage/QRWN3ZGK/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:/Users/assa/Zotero/storage/G4YA8TPI/1706.html:text/html},
}

@incollection{vedaldi_image-based_2020,
	location = {Cham},
	title = {Image-Based Table Recognition: Data, Model, and Evaluation},
	volume = {12366},
	isbn = {978-3-030-58588-4 978-3-030-58589-1},
	url = {https://link.springer.com/10.1007/978-3-030-58589-1_34},
	shorttitle = {Image-Based Table Recognition},
	abstract = {Important information that relates to a speciﬁc topic in a document is often organized in tabular format to assist readers with information retrieval and comparison, which may be diﬃcult to provide in natural language. However, tabular data in unstructured digital documents, e.g. Portable Document Format ({PDF}) and images, are difﬁcult to parse into structured machine-readable format, due to complexity and diversity in their structure and style. To facilitate image-based table recognition with deep learning, we develop and release the largest publicly available table recognition dataset {PubTabNet} (https://github. com/ibm-aur-nlp/{PubTabNet}.), containing 568k table images with corresponding structured {HTML} representation. {PubTabNet} is automatically generated by matching the {XML} and {PDF} representations of the scientiﬁc articles in {PubMed} {CentralTM} Open Access Subset ({PMCOA}). We also propose a novel attention-based encoder-dual-decoder ({EDD}) architecture that converts images of tables into {HTML} code. The model has a structure decoder which reconstructs the table structure and helps the cell decoder to recognize cell content. In addition, we propose a new Tree-Edit-Distance-based Similarity ({TEDS}) metric for table recognition, which more appropriately captures multi-hop cell misalignment and {OCR} errors than the pre-established metric. The experiments demonstrate that the {EDD} model can accurately recognize complex tables solely relying on the image representation, outperforming the state-of-the-art by 9.7\% absolute {TEDS} score.},
	pages = {564--580},
	booktitle = {Computer Vision – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Zhong, Xu and {ShafieiBavani}, Elaheh and Jimeno Yepes, Antonio},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	urldate = {2022-01-07},
	date = {2020},
	langid = {english},
	doi = {10.1007/978-3-030-58589-1_34},
	note = {Series Title: Lecture Notes in Computer Science},
	file = {Zhong et al. - 2020 - Image-Based Table Recognition Data, Model, and Ev.pdf:/Users/assa/Zotero/storage/RKYR32V4/Zhong et al. - 2020 - Image-Based Table Recognition Data, Model, and Ev.pdf:application/pdf},
}

@inproceedings{paliwal_tablenet_2019,
	title = {{TableNet}: Deep Learning Model for End-to-end Table Detection and Tabular Data Extraction from Scanned Document Images},
	doi = {10.1109/ICDAR.2019.00029},
	shorttitle = {{TableNet}},
	abstract = {With the widespread use of mobile phones and scanners to photograph and upload documents, the need for extracting the information trapped in unstructured document images such as retail receipts, insurance claim forms and financial invoices is becoming more acute. A major hurdle to this objective is that these images often contain information in the form of tables and extracting data from tabular sub-images presents a unique set of challenges. This includes accurate detection of the tabular region within an image, and subsequently detecting and extracting information from the rows and columns of the detected table. While some progress has been made in table detection, extracting the table contents is still a challenge since this involves more fine grained table structure (rows \& columns) recognition. Prior approaches have attempted to solve the table detection and structure recognition problems independently using two separate models. In this paper, we propose {TableNet}: a novel end-to-end deep learning model for both table detection and structure recognition. The model exploits the interdependence between the twin tasks of table detection and table structure recognition to segment out the table and column regions. This is followed by semantic rule-based row extraction from the identified tabular sub-regions. The proposed model and extraction approach was evaluated on the publicly available {ICDAR} 2013 and Marmot Table datasets obtaining state of the art results. Additionally, we demonstrate that feeding additional semantic features further improves model performance and that the model exhibits transfer learning across datasets. Another contribution of this paper is to provide additional table structure annotations for the Marmot data, which currently only has annotations for table detection.},
	eventtitle = {2019 International Conference on Document Analysis and Recognition ({ICDAR})},
	pages = {128--133},
	booktitle = {2019 International Conference on Document Analysis and Recognition ({ICDAR})},
	author = {Paliwal, Shubham Singh and D, Vishwanath and Rahul, Rohit and Sharma, Monika and Vig, Lovekesh},
	date = {2019-09},
	note = {{ISSN}: 2379-2140},
	keywords = {Training, Semantics, Deep learning, Feature extraction, Convolution, Data mining, Decoding, Table Detection, Table Structure Recognition, Scanned Documents, Information Extraction},
	file = {Version soumise:/Users/assa/Zotero/storage/HYSX573E/Paliwal et al. - 2019 - TableNet Deep Learning Model for End-to-end Table.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/assa/Zotero/storage/SFM7QY6Q/8978013.html:text/html},
}

@inproceedings{li_tablebank_2020,
	location = {Marseille, France},
	title = {{TableBank}: Table Benchmark for Image-based Table Detection and Recognition},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.236},
	shorttitle = {{TableBank}},
	abstract = {We present {TableBank}, a new image-based table detection and recognition dataset built with novel weak supervision from Word and Latex documents on the internet. Existing research for image-based table detection and recognition usually fine-tunes pre-trained models on out-of-domain data with a few thousand human-labeled examples, which is difficult to generalize on real-world applications. With {TableBank} that contains 417K high quality labeled tables, we build several strong baselines using state-of-the-art models with deep neural networks. We make {TableBank} publicly available and hope it will empower more deep learning approaches in the table detection and recognition task. The dataset and models can be downloaded from https://github.com/doc-analysis/{TableBank}.},
	eventtitle = {{LREC} 2020},
	pages = {1918--1925},
	booktitle = {Proceedings of the 12th Language Resources and Evaluation Conference},
	publisher = {European Language Resources Association},
	author = {Li, Minghao and Cui, Lei and Huang, Shaohan and Wei, Furu and Zhou, Ming and Li, Zhoujun},
	urldate = {2022-01-07},
	date = {2020-05},
}

@inproceedings{gilani_table_2017,
	title = {Table Detection Using Deep Learning},
	volume = {01},
	doi = {10.1109/ICDAR.2017.131},
	abstract = {Table detection is a crucial step in many document analysis applications as tables are used for presenting essential information to the reader in a structured manner. It is a hard problem due to varying layouts and encodings of the tables. Researchers have proposed numerous techniques for table detection based on layout analysis of documents. Most of these techniques fail to generalize because they rely on hand engineered features which are not robust to layout variations. In this paper, we have presented a deep learning based method for table detection. In the proposed method, document images are first pre-processed. These images are then fed to a Region Proposal Network followed by a fully connected neural network for table detection. The proposed method works with high precision on document images with varying layouts that include documents, research papers, and magazines. We have done our evaluations on publicly available {UNLV} dataset where it beats Tesseract's state of the art table detection system by a significant margin.},
	eventtitle = {2017 14th {IAPR} International Conference on Document Analysis and Recognition ({ICDAR})},
	pages = {771--776},
	booktitle = {2017 14th {IAPR} International Conference on Document Analysis and Recognition ({ICDAR})},
	author = {Gilani, Azka and Qasim, Shah Rukh and Malik, Imran and Shafait, Faisal},
	date = {2017-11},
	note = {{ISSN}: 2379-2140},
	keywords = {Layout, Text analysis, Machine learning, Portable document format, Proposals, Deep Learning, Document Analysis, Optical character recognition software, Table Detection, Transforms},
	file = {IEEE Xplore Abstract Record:/Users/assa/Zotero/storage/3SGBAA3F/8270062.html:text/html},
}

@article{redmon_you_2016,
	title = {You Only Look Once: Unified, Real-Time Object Detection},
	url = {http://arxiv.org/abs/1506.02640},
	shorttitle = {You Only Look Once},
	abstract = {We present {YOLO}, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base {YOLO} model processes images in real-time at 45 frames per second. A smaller version of the network, Fast {YOLO}, processes an astounding 155 frames per second while still achieving double the {mAP} of other real-time detectors. Compared to state-of-the-art detection systems, {YOLO} makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, {YOLO} learns very general representations of objects. It outperforms all other detection methods, including {DPM} and R-{CNN}, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
	journaltitle = {{arXiv}:1506.02640 [cs]},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	urldate = {2022-01-08},
	date = {2016-05-09},
	eprinttype = {arxiv},
	eprint = {1506.02640},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/assa/Zotero/storage/GA2V4E5X/Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf:application/pdf;arXiv.org Snapshot:/Users/assa/Zotero/storage/BBGF2BH4/1506.html:text/html},
}

@incollection{ziomek_glosat_2021,
	location = {New York, {NY}, {USA}},
	title = {{GloSAT} Historical Measurement Table Dataset: Enhanced Table Structure Recognition Annotation for Downstream Historical Data Rescue},
	isbn = {978-1-4503-8690-6},
	url = {https://doi.org/10.1145/3476887.3476890},
	shorttitle = {{GloSAT} Historical Measurement Table Dataset},
	abstract = {Understanding and extracting tables from documents is a research problem that has been studied for decades. Table structure recognition is the labelling of components within a detected table, which can be detected automatically or manually provided. This paper presents the {GloSAT} historical measurement table dataset designed to train table structure recognition models for use in downstream historical data rescue applications. The dataset contains 500 scanned and manually annotated images of pages from meteorological measurement logbooks. We enhance standard full table and individual cell annotations by adding additional annotations for headings, headers, and table bodies. We also provide annotations for coarse segmentation cells consisting of multiple data cells logically grouped by ruling lines of ink or whitespace in the table, which often represent data cells that are semantically grouped. Our dataset annotations are provided in {VOC}2007 and {ICDAR}-2019 Competition on Table Detection and Recognition ({cTDaR}-19) {XML} formats, and our dataset can easily be aggregated with the {cTDaR}-19 dataset. We report results running a series of benchmark algorithms on our new dataset, concluding that post-processing is very important for performance, and that page style is not as significant a feature as table type on model performance.},
	pages = {49--54},
	booktitle = {The 6th International Workshop on Historical Document Imaging and Processing},
	publisher = {Association for Computing Machinery},
	author = {Ziomek, Juliusz and Middleton, Stuart E.},
	urldate = {2022-01-08},
	date = {2021-09-05},
	keywords = {Deep Learning, Document Layout Analysis, Historical Documents, Image Processing, Measurements, Table Structure Recognition},
	file = {Full Text PDF:/Users/assa/Zotero/storage/AEKPTHQ4/Ziomek et Middleton - 2021 - GloSAT Historical Measurement Table Dataset Enhan.pdf:application/pdf},
}

@inproceedings{gobel_icdar_2013,
	title = {{ICDAR} 2013 Table Competition},
	doi = {10.1109/ICDAR.2013.292},
	abstract = {Table understanding is a well studied problem in document analysis, and many academic and commercial approaches have been developed to recognize tables in several document formats, including plain text, scanned page images and born-digital, object-based formats such as {PDF}. Despite the abundance of these techniques, an objective comparison of their performance is still missing. The Table Competition held in the context of {ICDAR} 2013 is our first attempt at objectively evaluating these techniques against each other in a standardized way, across several input formats. The competition independently addresses three problems: (i) table location, (ii) table structure recognition, and (iii) these two tasks combined. We received results from seven academic systems, which we have also compared against four commercial products. This paper presents our findings.},
	eventtitle = {2013 12th International Conference on Document Analysis and Recognition},
	pages = {1449--1453},
	booktitle = {2013 12th International Conference on Document Analysis and Recognition},
	author = {Göbel, Max and Hassan, Tamir and Oro, Ermelinda and Orsi, Giorgio},
	date = {2013-08},
	note = {{ISSN}: 2379-2140},
	keywords = {Text analysis, Training, document analysis, Portable document format, born-digital {PDF}, document understanding, Educational institutions, Electronic mail, {HTML}, Measurement, {PDF}, table location, table recognition, table structure recognition, table understanding},
	file = {IEEE Xplore Abstract Record:/Users/assa/Zotero/storage/G3R3IZ4B/6628853.html:text/html},
}

@article{li_docbank_2020,
	title = {{DocBank}: A Benchmark Dataset for Document Layout Analysis},
	url = {https://arxiv.org/abs/2006.01038v3},
	shorttitle = {{DocBank}},
	abstract = {Document layout analysis usually relies on computer vision models to understand documents while ignoring textual information that is vital to capture. Meanwhile, high quality labeled datasets with both visual and textual information are still insufficient. In this paper, we present {\textbackslash}textbf\{{DocBank}\}, a benchmark dataset that contains 500K document pages with fine-grained token-level annotations for document layout analysis. {DocBank} is constructed using a simple yet effective way with weak supervision from the {\textbackslash}{LaTeX}\{\} documents available on the {arXiv}.com. With {DocBank}, models from different modalities can be compared fairly and multi-modal approaches will be further investigated and boost the performance of document layout analysis. We build several strong baselines and manually split train/dev/test sets for evaluation. Experiment results show that models trained on {DocBank} accurately recognize the layout information for a variety of documents. The {DocBank} dataset is publicly available at {\textbackslash}url\{https://github.com/doc-analysis/{DocBank}\}.},
	author = {Li, Minghao and Xu, Yiheng and Cui, Lei and Huang, Shaohan and Wei, Furu and Li, Zhoujun and Zhou, Ming},
	urldate = {2022-01-08},
	date = {2020-06-01},
	langid = {english},
	file = {Snapshot:/Users/assa/Zotero/storage/SSNMC9I3/2006.html:text/html;Full Text PDF:/Users/assa/Zotero/storage/Y7F8ALDB/Li et al. - 2020 - DocBank A Benchmark Dataset for Document Layout A.pdf:application/pdf},
}

@inproceedings{schreiber_deepdesrt_2017,
	location = {Kyoto},
	title = {{DeepDeSRT}: Deep Learning for Detection and Structure Recognition of Tables in Document Images},
	isbn = {978-1-5386-3586-5},
	url = {http://ieeexplore.ieee.org/document/8270123/},
	doi = {10.1109/ICDAR.2017.192},
	shorttitle = {{DeepDeSRT}},
	abstract = {This paper presents a novel end-to-end system for table understanding in document images called {DeepDeSRT}. In particular, the contribution of {DeepDeSRT} is two-fold. First, it presents a deep learning-based solution for table detection in document images. Secondly, it proposes a novel deep learningbased approach for table structure recognition, i.e. identifying rows, columns, and cell positions in the detected tables. In contrast to existing rule-based methods, which rely on heuristics or additional {PDF} metadata (like, for example, print instructions, character bounding boxes, or line segments), the presented system is data-driven and does not need any heuristics or metadata to detect as well as to recognize tabular structures in document images. Furthermore, in contrast to most existing table detection and structure recognition methods, which are applicable only to {PDFs}, {DeepDeSRT} processes document images, which makes it equally suitable for born-digital {PDFs} (as they can automatically be converted into images) as well as even harder problems, e.g. scanned documents. To gauge the performance of {DeepDeSRT}, the system is evaluated on the publicly available {ICDAR} 2013 table competition dataset containing 67 documents with 238 pages overall. Evaluation results reveal that {DeepDeSRT} outperforms state-of-the-art methods for table detection and structure recognition and achieves F1-measures of 96.77\% and 91.44\% for table detection and structure recognition, respectively. Additionally, {DeepDeSRT} is evaluated on a closed dataset from a real use case of a major European aviation company comprising documents which are highly unlike those in {ICDAR} 2013. Tested on a randomly selected sample from this dataset, {DeepDeSRT} achieves high detection accuracy for tables which demonstrates the sound generalization capabilities of our system.},
	eventtitle = {2017 14th {IAPR} International Conference on Document Analysis and Recognition ({ICDAR})},
	pages = {1162--1167},
	booktitle = {2017 14th {IAPR} International Conference on Document Analysis and Recognition ({ICDAR})},
	publisher = {{IEEE}},
	author = {Schreiber, Sebastian and Agne, Stefan and Wolf, Ivo and Dengel, Andreas and Ahmed, Sheraz},
	urldate = {2022-01-08},
	date = {2017-11},
	langid = {english},
	file = {Schreiber et al. - 2017 - DeepDeSRT Deep Learning for Detection and Structu.pdf:/Users/assa/Zotero/storage/YPPRY6P9/Schreiber et al. - 2017 - DeepDeSRT Deep Learning for Detection and Structu.pdf:application/pdf},
}

@inproceedings{sun_faster_2019,
	title = {Faster R-{CNN} Based Table Detection Combining Corner Locating},
	doi = {10.1109/ICDAR.2019.00212},
	abstract = {Table detection in document images has achieved remarkable improvement. However, there is still a problem of inaccurate table boundary locating. This paper proposes Faster R-{CNN} based table detection combining corner locating method. Firstly, coarse table detection and corner locating are implemented through Faster R-{CNN} network. Secondly, those corners belonging to the same tables are grouped by coordinate matching. At the same time, unreliable corners are filtered. Finally, table boundaries are adjusted and refined by corresponding corner group. The proposed method improves the precision of table boundary locating at pixel-level. Experiment results show that our method effectively improves the precision of table detection. It achieves an F-measure of 94.9\% on {ICDAR}2017 {POD} dataset. Compared with traditional Faster R-{CNN} method, our method increases by 2.8\% in F-measure and 2.1\% at pixellevel localization.},
	eventtitle = {2019 International Conference on Document Analysis and Recognition ({ICDAR})},
	pages = {1314--1319},
	booktitle = {2019 International Conference on Document Analysis and Recognition ({ICDAR})},
	author = {Sun, Ningning and Zhu, Yuanping and Hu, Xiaoming},
	date = {2019-09},
	note = {{ISSN}: 2379-2140},
	keywords = {Image segmentation, Layout, Semantics, Deep learning, Feature extraction, Table detection, Proposals, Corner locating, Faster R-{CNN}, Reliability},
	file = {IEEE Xplore Abstract Record:/Users/assa/Zotero/storage/KASBXMLH/8978063.html:text/html},
}

@article{redmon_yolov3_2018,
	title = {{YOLOv}3: An Incremental Improvement},
	url = {http://arxiv.org/abs/1804.02767},
	shorttitle = {{YOLOv}3},
	abstract = {We present some updates to {YOLO}! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 {YOLOv}3 runs in 22 ms at 28.2 {mAP}, as accurate as {SSD} but three times faster. When we look at the old .5 {IOU} {mAP} detection metric {YOLOv}3 is quite good. It achieves 57.9 {mAP}@50 in 51 ms on a Titan X, compared to 57.5 {mAP}@50 in 198 ms by {RetinaNet}, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/},
	journaltitle = {{arXiv}:1804.02767 [cs]},
	author = {Redmon, Joseph and Farhadi, Ali},
	urldate = {2022-01-08},
	date = {2018-04-08},
	eprinttype = {arxiv},
	eprint = {1804.02767},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/assa/Zotero/storage/2A8NFCIH/Redmon et Farhadi - 2018 - YOLOv3 An Incremental Improvement.pdf:application/pdf;arXiv.org Snapshot:/Users/assa/Zotero/storage/2ET4IYR5/1804.html:text/html},
}

@article{wang_deep_2020,
	title = {Deep High-Resolution Representation Learning for Visual Recognition},
	url = {http://arxiv.org/abs/1908.07919},
	abstract = {High-resolution representations are essential for position-sensitive vision problems, such as human pose estimation, semantic segmentation, and object detection. Existing state-of-the-art frameworks first encode the input image as a low-resolution representation through a subnetwork that is formed by connecting high-to-low resolution convolutions {\textbackslash}emph\{in series\} (e.g., {ResNet}, {VGGNet}), and then recover the high-resolution representation from the encoded low-resolution representation. Instead, our proposed network, named as High-Resolution Network ({HRNet}), maintains high-resolution representations through the whole process. There are two key characteristics: (i) Connect the high-to-low resolution convolution streams {\textbackslash}emph\{in parallel\}; (ii) Repeatedly exchange the information across resolutions. The benefit is that the resulting representation is semantically richer and spatially more precise. We show the superiority of the proposed {HRNet} in a wide range of applications, including human pose estimation, semantic segmentation, and object detection, suggesting that the {HRNet} is a stronger backbone for computer vision problems. All the codes are available at{\textasciitilde}\{{\textbackslash}url\{https://github.com/{HRNet}\}\}.},
	journaltitle = {{arXiv}:1908.07919 [cs]},
	author = {Wang, Jingdong and Sun, Ke and Cheng, Tianheng and Jiang, Borui and Deng, Chaorui and Zhao, Yang and Liu, Dong and Mu, Yadong and Tan, Mingkui and Wang, Xinggang and Liu, Wenyu and Xiao, Bin},
	urldate = {2022-01-08},
	date = {2020-03-13},
	eprinttype = {arxiv},
	eprint = {1908.07919},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/assa/Zotero/storage/9C8H3D39/Wang et al. - 2020 - Deep High-Resolution Representation Learning for V.pdf:application/pdf;arXiv.org Snapshot:/Users/assa/Zotero/storage/UPRMSS24/1908.html:text/html},
}

@article{kavasidis_saliency-based_2018,
	title = {A Saliency-based Convolutional Neural Network for Table and Chart Detection in Digitized Documents},
	url = {http://arxiv.org/abs/1804.06236},
	abstract = {Deep Convolutional Neural Networks ({DCNNs}) have recently been applied successfully to a variety of vision and multimedia tasks, thus driving development of novel solutions in several application domains. Document analysis is a particularly promising area for {DCNNs}: indeed, the number of available digital documents has reached unprecedented levels, and humans are no longer able to discover and retrieve all the information contained in these documents without the help of automation. Under this scenario, {DCNNs} offers a viable solution to automate the information extraction process from digital documents. Within the realm of information extraction from documents, detection of tables and charts is particularly needed as they contain a visual summary of the most valuable information contained in a document. For a complete automation of visual information extraction process from tables and charts, it is necessary to develop techniques that localize them and identify precisely their boundaries.},
	journaltitle = {{arXiv}:1804.06236 [cs]},
	author = {Kavasidis, I. and Palazzo, S. and Spampinato, C. and Pino, C. and Giordano, D. and Giuffrida, D. and Messina, P.},
	urldate = {2022-01-08},
	date = {2018-04-17},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1804.06236},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Kavasidis et al. - 2018 - A Saliency-based Convolutional Neural Network for .pdf:/Users/assa/Zotero/storage/4GESMHTG/Kavasidis et al. - 2018 - A Saliency-based Convolutional Neural Network for .pdf:application/pdf},
}

@article{brown_language_2020,
	title = {Language Models are Few-Shot Learners},
	url = {http://arxiv.org/abs/2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many {NLP} tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current {NLP} systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train {GPT}-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, {GPT}-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. {GPT}-3 achieves strong performance on many {NLP} datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where {GPT}-3's few-shot learning still struggles, as well as some datasets where {GPT}-3 faces methodological issues related to training on large web corpora. Finally, we find that {GPT}-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of {GPT}-3 in general.},
	journaltitle = {{arXiv}:2005.14165 [cs]},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and {McCandlish}, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	urldate = {2022-01-08},
	date = {2020-07-22},
	eprinttype = {arxiv},
	eprint = {2005.14165},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/assa/Zotero/storage/LITTFYZV/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf;arXiv.org Snapshot:/Users/assa/Zotero/storage/YWIAN3X8/2005.html:text/html},
}

@article{ghasemi-gol_tabvec_2018,
	title = {{TabVec}: Table Vectors for Classification of Web Tables},
	url = {http://arxiv.org/abs/1802.06290},
	shorttitle = {{TabVec}},
	abstract = {There are hundreds of millions of tables in Web pages that contain useful information for many applications. Leveraging data within these tables is di cult because of the wide variety of structures, formats and data encoded in these tables. {TabVec} is an unsupervised method to embed tables into a vector space to support classi cation of tables into categories (entity, relational, matrix, list, and nondata) with minimal user intervention. {TabVec} deploys syntax and semantics of table cells, and embeds the structure of tables in a table vector space. This enables superior classi cation of tables even in the absence of domain annotations. Our evaluations in four real world domains show that {TabVec} improves classi cation accuracy by more than 20\% compared to three state of the art systems, and that those systems require signi cant in domain training to achieve good results.},
	journaltitle = {{arXiv}:1802.06290 [cs]},
	author = {Ghasemi-Gol, Majid and Szekely, Pedro},
	urldate = {2022-01-09},
	date = {2018-02-17},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1802.06290},
	keywords = {Computer Science - Information Retrieval},
	file = {Ghasemi-Gol et Szekely - 2018 - TabVec Table Vectors for Classification of Web Ta.pdf:/Users/assa/Zotero/storage/6I3BUR4L/Ghasemi-Gol et Szekely - 2018 - TabVec Table Vectors for Classification of Web Ta.pdf:application/pdf},
}

@inproceedings{siddiqui_rethinking_2019,
	title = {Rethinking Semantic Segmentation for Table Structure Recognition in Documents},
	doi = {10.1109/ICDAR.2019.00225},
	abstract = {Based on the recent advancements in the domain of semantic segmentation, Fully-Convolutional Networks ({FCN}) have been successfully applied for the task of table structure recognition in the past. We analyze the efficacy of semantic segmentation networks for this purpose and simplify the problem by proposing prediction tiling based on the consistency assumption which holds for tabular structures. For an image of dimensions H × W, we predict a single column for the rows (ŷrow ϵ H) and a predict a single row for the columns (ŷrow ϵ W). We use a dual-headed architecture where initial feature maps (from the encoder-decoder model) are shared while the last two layers generate class specific (row/column) predictions. This allows us to generate predictions using a single model for both rows and columns simultaneously, where previous methods relied on two separate models for inference. With the proposed method, we were able to achieve state-of-the-art results on {ICDAR}-13 image-based table structure recognition dataset with an average F-Measure of 92.39\% (91.90\% and 92.88\% F-Measure for rows and columns respectively). With the proposed method, we were able to achieve state-of-the-art results on {ICDAR}-13. The obtained results advocate that constraining the problem space in the case of {FCN} by imposing valid constraints can lead to significant performance gains.},
	eventtitle = {2019 International Conference on Document Analysis and Recognition ({ICDAR})},
	pages = {1397--1402},
	booktitle = {2019 International Conference on Document Analysis and Recognition ({ICDAR})},
	author = {Siddiqui, Shoaib Ahmed and Khan, Pervaiz Iqbal and Dengel, Andreas and Ahmed, Sheraz},
	date = {2019-09},
	note = {{ISSN}: 2379-2140},
	keywords = {Image segmentation, Task analysis, Semantics, Feature extraction, Data mining, Decoding, Measurement, Document Analysis, Table Structure Recognition, Table Understanding, Fully-Convolutional Networks ({FCN})},
	file = {IEEE Xplore Full Text PDF:/Users/assa/Zotero/storage/PFSJD97C/Siddiqui et al. - 2019 - Rethinking Semantic Segmentation for Table Structu.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/assa/Zotero/storage/TMGW7ZPN/8978088.html:text/html},
}

@online{noauthor_wan_2010,
	title = {{WAN} - Newspapers: 400 Years Young!},
	url = {https://web.archive.org/web/20100310235015/http://www.wan-press.org/article6476.html},
	shorttitle = {{WAN} - Newspapers},
	urldate = {2022-01-11},
	date = {2010-03-10},
	file = {Snapshot:/Users/assa/Zotero/storage/F7J5C87A/article6476.html:text/html},
}

@article{wenkel_confidence_2021,
	title = {Confidence Score: The Forgotten Dimension of Object Detection Performance Evaluation},
	volume = {21},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1424-8220/21/13/4350},
	doi = {10.3390/s21134350},
	shorttitle = {Confidence Score},
	abstract = {When deploying a model for object detection, a confidence score threshold is chosen to filter out false positives and ensure that a predicted bounding box has a certain minimum score. To achieve state-of-the-art performance on benchmark datasets, most neural networks use a rather low threshold as a high number of false positives is not penalized by standard evaluation metrics. However, in scenarios of Artificial Intelligence ({AI}) applications that require high confidence scores (e.g., due to legal requirements or consequences of incorrect detections are severe) or a certain level of model robustness is required, it is unclear which base model to use since they were mainly optimized for benchmark scores. In this paper, we propose a method to find the optimum performance point of a model as a basis for fairer comparison and deeper insights into the trade-offs caused by selecting a confidence score threshold.},
	pages = {4350},
	number = {13},
	journaltitle = {Sensors},
	author = {Wenkel, Simon and Alhazmi, Khaled and Liiv, Tanel and Alrshoud, Saud and Simon, Martin},
	urldate = {2022-01-11},
	date = {2021-01},
	langid = {english},
	note = {Number: 13
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {computer vision, confidence score, deep neural networks, object detection},
	file = {Full Text PDF:/Users/assa/Zotero/storage/HT4EBTHB/Wenkel et al. - 2021 - Confidence Score The Forgotten Dimension of Objec.pdf:application/pdf;Snapshot:/Users/assa/Zotero/storage/7GLGEJS4/htm.html:text/html},
}

@inproceedings{petit_handling_2018,
	location = {Cham},
	title = {Handling Missing Annotations for Semantic Segmentation with Deep {ConvNets}},
	isbn = {978-3-030-00889-5},
	doi = {10.1007/978-3-030-00889-5_3},
	series = {Lecture Notes in Computer Science},
	abstract = {Annotation of medical images for semantic segmentation is a very time consuming and difficult task. Moreover, clinical experts often focus on specific anatomical structures and thus, produce partially annotated images. In this paper, we introduce {SMILE}, a new deep convolutional neural network which addresses the issue of learning with incomplete ground truth. {SMILE} aims to identify ambiguous labels in order to ignore them during training, and don’t propagate incorrect or noisy information. A second contribution is {SMILEr} which uses {SMILE} as initialization for automatically relabeling missing annotations, using a curriculum strategy. Experiments on 3 organ classes (liver, stomach, pancreas) show the relevance of the proposed approach for semantic segmentation: with 70\% of missing annotations, {SMILEr} performs similarly as a baseline trained with complete ground truth annotations.},
	pages = {20--28},
	booktitle = {Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support},
	publisher = {Springer International Publishing},
	author = {Petit, Olivier and Thome, Nicolas and Charnoz, Arnaud and Hostettler, Alexandre and Soler, Luc},
	editor = {Stoyanov, Danail and Taylor, Zeike and Carneiro, Gustavo and Syeda-Mahmood, Tanveer and Martel, Anne and Maier-Hein, Lena and Tavares, João Manuel R.S. and Bradley, Andrew and Papa, João Paulo and Belagiannis, Vasileios and Nascimento, Jacinto C. and Lu, Zhi and Conjeti, Sailesh and Moradi, Mehdi and Greenspan, Hayit and Madabhushi, Anant},
	date = {2018},
	langid = {english},
	keywords = {Deep learning, Convolutional Neural Networks, Incomplete ground truth annotation, Medical images, Missing labels, Noisy labels},
	file = {Springer Full Text PDF:/Users/assa/Zotero/storage/QAYSBJVL/Petit et al. - 2018 - Handling Missing Annotations for Semantic Segmenta.pdf:application/pdf},
}

@inproceedings{almutairi_instance_2019-1,
	title = {Instance Segmentation of Newspaper Elements Using Mask R-{CNN}},
	doi = {10.1109/ICMLA.2019.00223},
	abstract = {Newspaper digitization has gained wide interest around the world. Archives of digitized newspapers contain a wealth of information that spans decades. To extract this abundance of information, optical character recognition ({OCR}) techniques are used. However, as a first step, the newspaper pages should be logically deconstructed into articles to gain meaningful knowledge. This is difficult due to the complex layout of newspapers and the various styles, shapes, and languages of newspaper articles. Newspaper pages also contain other elements besides articles, such as advertisements that come in multiple shapes and forms, and top headers that contain information about the newspaper's issue and page. Therefore, it is important to detect these elements before information extraction begins. In this paper, we present a deep learning solution for the problem of newspaper page semantic segmentation of the main newspaper elements (articles, advertisements, and page headers). We employed the instance segmentation method mask R-{CNN} mask\_rcnn to create a language-agnostic model that logically deconstructs a newspaper page raw image into its main elements based only on its visual features. We show the results of experiments that display the accuracy and robustness of our model.},
	eventtitle = {2019 18th {IEEE} International Conference On Machine Learning And Applications ({ICMLA})},
	pages = {1371--1375},
	booktitle = {2019 18th {IEEE} International Conference On Machine Learning And Applications ({ICMLA})},
	author = {Almutairi, Abdullah and Almashan, Meshal},
	date = {2019-12},
	keywords = {Image segmentation, Layout, Training, Feature extraction, Machine learning, newspaper article segmentation, semantic segmentation, deep learning, mask R-{CNN}, Shape, Training data},
	file = {IEEE Xplore Full Text PDF:/Users/assa/Zotero/storage/MISSVQF4/Almutairi et Almashan - 2019 - Instance Segmentation of Newspaper Elements Using .pdf:application/pdf;IEEE Xplore Abstract Record:/Users/assa/Zotero/storage/57C4DMY9/8999273.html:text/html},
}

@article{weber_strassburg_2006,
	title = {Strassburg, 1605: The Origins of the Newspaper in Europe},
	volume = {24},
	issn = {0266-3554},
	url = {https://doi.org/10.1191/0266355406gh380oa},
	doi = {10.1191/0266355406gh380oa},
	shorttitle = {Strassburg, 1605},
	abstract = {English-language examinations of the history of the press often begin with the 1620s or the 1640s, when this new medium began to be more widespread in England. On the continent, however, around the year 1600 all the necessary technical, infrastructural and communication elements were already in place for the development of the modern newspaper. Book-printing techniques enabled the mass production of news reports; a regular relay post system, available to the general public, served the needs of a network of professional correspondents from around the world. The time was right for the first periodical news sheet, which appeared in Strassburg in 1605. The change from hand-copied to printed newspapers was of far-reaching significance, as the mass circulation of news gave regular publicity to the events and personalities of political life. By the 1620s a variety of newspapers were circulating in central Europe, and in the second half of the seventeenth century newspapers were the most widely read secular material. They provided a seedbed for the broadening political education which fostered the development of the Enlightenment.},
	pages = {387--412},
	number = {3},
	journaltitle = {German History},
	shortjournal = {German History},
	author = {Weber, Johannes},
	urldate = {2022-01-13},
	date = {2006-07-01},
	file = {Full Text PDF:/Users/assa/Zotero/storage/SM4DT7FD/Weber - 2006 - Strassburg, 1605 The Origins of the Newspaper in .pdf:application/pdf;Snapshot:/Users/assa/Zotero/storage/IVZD4GHE/597682.html:text/html},
}

@article{barman_historical_2019,
	title = {Historical newspaper semantic segmentation using visual and textual features},
	url = {http://infoscience.epfl.ch/record/271306},
	abstract = {Mass digitization and the opening of digital libraries gave access to a huge amount of historical newspapers. In order to bring structure into these documents, current techniques generally proceed in two distinct steps. First, they segment the digitized images into generic articles and then classify the text of the articles into finer-grained categories. Unfortunately, by losing the link between layout and text, these two steps are not able to account for the fact that newspaper content items have distinctive visual features. This project proposes two main novelties. Firstly, it introduces the idea of merging the segmentation and classification steps, resulting in a fine- grained semantic segmentation of newspapers images. Secondly, it proposes to use textual features under the form of embeddings maps at segmentation step. The semantic segmentation with four categories (feuilleton, weather forecast, obituary, and stock exchange table) is done using a fully convolutional neural network and reaches a {mIoU} of 79.3\%. The introduction of embeddings maps improves the overall performances by 3\% and the generalization across time and newspapers by 8\% and 12\%, respectively. This shows a strong potential to consider the semantic aspect in the segmentation of newspapers and to use textual features to improve generalization.},
	pages = {74},
	author = {Barman, Raphaël},
	date = {2019},
}

@article{zanibbi_survey_2004,
	title = {A survey of table recognition},
	volume = {7},
	doi = {10.1007/s10032-004-0120-9},
	abstract = {Table characteristics vary widely. Consequently, a great variety of computational approaches have been applied to table recognition. In this survey, the table recognition literature is presented as an interaction of table models, observations, transformations, and inferences. A table model defines the physical and logical structure of tables; the model is used to detect tables and to analyze and decompose the detected tables. Observations perform feature measurements and data lookup, transformations alter or restructure data, and inferences generate and test hypotheses. This presentation clarifies both the decisions made by a table recognizer and the assumptions and inferencing techniques that underlie these decisions.},
	pages = {1--16},
	journaltitle = {{IJDAR}},
	shortjournal = {{IJDAR}},
	author = {Zanibbi, Richard and Blostein, Dorothea and Cordy, James},
	date = {2004-03-01},
	file = {Full Text PDF:/Users/assa/Zotero/storage/ABPAHY4U/Zanibbi et al. - 2004 - A survey of table recognition.pdf:application/pdf},
}